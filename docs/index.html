<?xml version="1.0" encoding="utf-8"?>
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2020-05-04 Mon 14:35 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Project Log</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Christoffer Arvidsson" />
<meta name="description" content="This document logs project progress for a bachelor thesis at Chalmers University in Computer Science."
 />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/styles/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/styles/readtheorg/js/readtheorg.js"></script>
<link rel="stylesheet" type="text/css" href="./style.css" />
<script type="text/javascript">
// @license magnet:?xt=urn:btih:1f739d935676111cfff4b4693e3816e664797050&amp;dn=gpl-3.0.txt GPL-v3-or-Later
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.cacheClassElem = elem.className;
         elem.cacheClassTarget = target.className;
         target.className = "code-highlighted";
         elem.className   = "code-highlighted";
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(elem.cacheClassElem)
         elem.className = elem.cacheClassElem;
       if(elem.cacheClassTarget)
         target.className = elem.cacheClassTarget;
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Project Log</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orga137854">Introduction</a></li>
<li><a href="#orgb1c5f44">Week 14 and 15</a>
<ul>
<li><a href="#orgc78acb1">Report</a></li>
<li><a href="#org0d4d503">Exhibition</a></li>
<li><a href="#org30b5448">Structure generator</a></li>
<li><a href="#orgaf36af3">Audio generator</a></li>
<li><a href="#org877051d">Summary of each member</a></li>
<li><a href="#org702c52e">Next few weeks</a></li>
</ul>
</li>
<li><a href="#org3a75f81">Week 13</a>
<ul>
<li><a href="#orgc73161e">Audio generation</a>
<ul>
<li><a href="#org718c31e">Pitch conditioning</a></li>
<li><a href="#org7c502d1">Constant Q transform</a></li>
<li><a href="#orgc41851c">Phase channel and instantaneous frequency</a></li>
</ul>
</li>
<li><a href="#org9bb589f">Structure generation</a></li>
<li><a href="#orgf2a1c98">Combining models</a></li>
<li><a href="#org24dca14">Report</a></li>
<li><a href="#org03c3695">Summary of each member</a></li>
<li><a href="#orgc0a5153">Last weeks&rsquo; goals</a></li>
<li><a href="#org79b1f29">Next week</a></li>
</ul>
</li>
<li><a href="#org19f350e">Week 12</a>
<ul>
<li><a href="#orga462daf">Progress</a>
<ul>
<li><a href="#orgcf528dd">Pitch conditioning and Auxiliary Classifier</a></li>
<li><a href="#org5e6e929">Training</a></li>
<li><a href="#orgd58ac58">Current Results of Note Generation with Conditioning and Classifier</a></li>
<li><a href="#orgbdd7e87">Pitch experiments</a></li>
<li><a href="#org9348fab">Report</a></li>
</ul>
</li>
<li><a href="#org67eebee">Last weeks&rsquo; goals</a></li>
<li><a href="#org10f215c">Summary of each member</a></li>
<li><a href="#org255ccb6">Next week</a></li>
</ul>
</li>
<li><a href="#org6a9314c">Week 11</a>
<ul>
<li><a href="#org69520c4">Progress</a>
<ul>
<li><a href="#org01dfbc1">Report</a></li>
<li><a href="#org3db2563">Note generation</a></li>
<li><a href="#orgcf29d37">Structure generation</a></li>
<li><a href="#orgced427b">Training</a></li>
</ul>
</li>
<li><a href="#orga315e94">Last weeks&rsquo; goals</a></li>
<li><a href="#orgece8d75">Summary of each member</a></li>
<li><a href="#org0cde2ee">Next week</a></li>
</ul>
</li>
<li><a href="#org5eefd09">Week 10</a>
<ul>
<li><a href="#orgbf21f6e">Progress</a>
<ul>
<li><a href="#org081c9f6">Report</a></li>
<li><a href="#org4562200">Note Generation</a></li>
<li><a href="#orge358210">Structure Generation</a></li>
<li><a href="#orgaaf1e63">Bayes Training</a></li>
</ul>
</li>
<li><a href="#org7ddc5b3">Summary of each member</a></li>
<li><a href="#org14d775d">Next week</a></li>
</ul>
</li>
<li><a href="#orgeefca24">Week 9</a>
<ul>
<li><a href="#orga6dd3f7">Progress</a>
<ul>
<li><a href="#orgdbc0adf">Report</a></li>
<li><a href="#orgc00579a">Shift of focus</a></li>
<li><a href="#org8613abf">VAE</a></li>
<li><a href="#orga6c697c">Transformer</a></li>
<li><a href="#org8710a57">Training resources</a></li>
<li><a href="#org22483cd">Meetings</a></li>
<li><a href="#org97e0fbf">Exam week</a></li>
</ul>
</li>
<li><a href="#org098e691">Next week</a></li>
</ul>
</li>
<li><a href="#orge821b23">Week 8</a>
<ul>
<li><a href="#org8eb0fb6">Progress</a>
<ul>
<li><a href="#org63a413f">Training resources</a></li>
<li><a href="#orgd3eeb1d">Meetings</a></li>
<li><a href="#org8129d82">Exam week</a></li>
<li><a href="#orgcace8b9">MIDI framework</a></li>
</ul>
</li>
<li><a href="#org5b81ec6">Summary of each member</a></li>
<li><a href="#org28c1eb4">Next week</a></li>
</ul>
</li>
<li><a href="#orgd20b154">Week 7</a>
<ul>
<li><a href="#org0231d4f">Important info</a></li>
<li><a href="#org708f736">Progress</a>
<ul>
<li><a href="#org4cf62f8">Presentation</a></li>
<li><a href="#org06a5f1c">SpecGAN</a></li>
<li><a href="#org30cddb7">New model proposal by Elias</a></li>
<li><a href="#orgbb7c7d4">Transformer and MIDI</a></li>
<li><a href="#org343f1a9">Problems</a></li>
</ul>
</li>
<li><a href="#orgbd00766">Summary of each member</a></li>
<li><a href="#org3ea7b85">Next week</a></li>
</ul>
</li>
<li><a href="#orge18c00b">Week 6</a>
<ul>
<li><a href="#orgae72b41">Project so far</a>
<ul>
<li><a href="#org2aafff1">WaveRNN</a></li>
<li><a href="#org3beb8a1">SpecGAN</a></li>
<li><a href="#org2970ca1">WaveNet</a></li>
<li><a href="#orgfe5f82e">Problems</a></li>
</ul>
</li>
<li><a href="#org756ed74">Meetings and workshops</a></li>
<li><a href="#orga3ee5db">Summary of each member</a></li>
<li><a href="#org4140a1f">Next week</a></li>
</ul>
</li>
<li><a href="#org8505c09">Week 5</a>
<ul>
<li><a href="#orga28fd66">Project plan review</a></li>
<li><a href="#orged52b2c">Project so far</a>
<ul>
<li><a href="#org8383d61">WaveNet</a></li>
<li><a href="#org2853281">SpecGAN</a></li>
</ul>
</li>
<li><a href="#org82e3527">Meetings and workshops</a></li>
<li><a href="#orgb19c805">Summary of each member</a></li>
<li><a href="#orgb586e02">Next week</a></li>
</ul>
</li>
<li><a href="#orgbf4bb2a">Week 4</a>
<ul>
<li><a href="#orgf932437">Time log warning</a></li>
<li><a href="#org3c29c23">Regarding project log feedback</a></li>
<li><a href="#org417b8aa">Meetings and workshops</a></li>
<li><a href="#org77f7527">Project so far</a></li>
<li><a href="#org5f69de4">Summary of each member</a></li>
</ul>
</li>
<li><a href="#org3789d5d">Week 3</a>
<ul>
<li><a href="#orgc9e65b8">Problems encountered</a></li>
</ul>
</li>
<li><a href="#org34ca3c8">Week 2</a></li>
<li><a href="#org2978513">Week 1</a></li>
</ul>
</div>
</div>


<div id="outline-container-orga137854" class="outline-2">
<h2 id="orga137854">Introduction</h2>
<div class="outline-text-2" id="text-orga137854">
<p>
This page is a shared log detailing the work the group has done and what
challenges and problems it has encountered. For the most part, it&rsquo;s updated
weekly unless some big change happens.
</p>

<p>
A time log can be found at Google Sheets: <a href="https://bit.ly/2PRKRl8">https://bit.ly/2PRKRl8</a>
</p>

<p>
The time log has exact tasks worked on each entry, for each
</p>
</div>
</div>
<div id="outline-container-orgb1c5f44" class="outline-2">
<h2 id="orgb1c5f44">Week 14 and 15</h2>
<div class="outline-text-2" id="text-orgb1c5f44">
<p>
We felt it appropriate to spend more time on the report and demo parts of the
project rather than documenting this log. Most of the past two weeks have been
writing the report. That said, we are doing some developing and training in the
background to try and fine tune our results, particularly audio quality.
</p>
</div>
<div id="outline-container-orgc78acb1" class="outline-3">
<h3 id="orgc78acb1">Report</h3>
<div class="outline-text-3" id="text-orgc78acb1">
<p>
To efficiently work on the report from home, we have many meetings and work in
sprints. This means we spend one day of the week finalizing our changes
(mondays), and one day to plan our sprints. Planning our sprints involves
everyone (including supervisor) reading the report, commenting, followed by the
group creating issues while discussing. Issues are simple, quick fixes that can
usually be solved in a few hours.
</p>

<p>
We started this around week 13, but now a few weeks later, it has definitely
sped up writing the report. We also feel the quality of everything has gone up
significantly and it makes it very easy to just pick something to work on,
without needing to meet the rest of the group.
</p>

<p>
As of right now, we are almost done with the report. We would have been done
sooner if training wasn&rsquo;t a bottle neck for the results section, but not much
can be done about that.
</p>
</div>
</div>

<div id="outline-container-org0d4d503" class="outline-3">
<h3 id="org0d4d503">Exhibition</h3>
<div class="outline-text-3" id="text-org0d4d503">
<p>
Elias and Cao wrote a script for the exhibition. We have a draft recording of
it, but will probably record again when we are entirely sure it&rsquo;s perfect.
</p>
</div>
</div>
<div id="outline-container-org30b5448" class="outline-3">
<h3 id="org30b5448">Structure generator</h3>
<div class="outline-text-3" id="text-org30b5448">
<p>
Training the structure generator takes a lot of time, and it has to be done in
parallel with training audio generation. As of writing this, the structure
generator is by far the most successful component in the system, performing very
well (for a bachelor thesis anyway). We consider this model done being trained,
though it could certainly improve with more training.
</p>
</div>
</div>

<div id="outline-container-orgaf36af3" class="outline-3">
<h3 id="orgaf36af3">Audio generator</h3>
<div class="outline-text-3" id="text-orgaf36af3">
<p>
The audio generator is not great. We have spent little time improving it these
past two weeks. The improvements we are doing mostly revolve around
hyperparameters, transform changes and experimentation with entire new modes.
</p>

<p>
Progressive GAN is something Eric implemented that seems promising. Basically it
is NVidia&rsquo;s way of progressively scaling up the resolution until a target
resolution is held. Our theory is that it optimizes training significanly since
it in stages adds detail. This model is currently being trained on guitar and
keyboard.
</p>

<p>
At the same time as above, we are working on generating the phase components as
a separate channel to magnitude. All models use this now, but the most trained
result still rebuilds it using Griffin-Lim, which is slow and provides bad
results for audio.
</p>
</div>
</div>

<div id="outline-container-org877051d" class="outline-3">
<h3 id="org877051d">Summary of each member</h3>
<div class="outline-text-3" id="text-org877051d">
<p>
We have all been working all over the report, picking issues from a backlog as detailed above. Still, here are the areas we are most responsible for (I reiterate: people have worked everywhere, not just what is listed here):
</p>
<ul class="org-ul">
<li><b>Christoffer</b>:
<ul class="org-ul">
<li>Theory &#x2013; signal processing</li>
<li>Design &#x2013; System overview and audio generation</li>
<li>Implementation &#x2013; audio generation</li>
<li>Results &#x2013; (not a lot currently)</li>
</ul></li>
<li><b>Eric</b>:
<ul class="org-ul">
<li><b>PROJECT</b>: Been training transformer and prog GAN</li>
<li>Design &#x2013; melody generation</li>
<li>Implementation &#x2013; melody generator</li>
</ul></li>
<li><b>Carl</b>:
<ul class="org-ul">
<li>Cover pages</li>
<li>Figures across the report</li>
<li>Design &#x2013; renderer</li>
<li>Implementation &#x2013; renderer</li>
</ul></li>
<li><b>Lovisa</b>:
<ul class="org-ul">
<li>Introduction</li>
<li>Theory regarding machine learning</li>
<li>Discussion &#x2013; section structure</li>
<li>Conclusion</li>
<li>Report language</li>
</ul></li>
<li><b>Cao</b>:
<ul class="org-ul">
<li>Theory regarding machine learning</li>
<li>Edits everywhere</li>
</ul></li>
<li><b>Elias</b>:
<ul class="org-ul">
<li>Introduction</li>
<li>Theory regarding VAEs and transformers</li>
<li>Discussion &#x2013; alternative models</li>
<li>Bibliographic notes</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org702c52e" class="outline-3">
<h3 id="org702c52e">Next few weeks</h3>
<div class="outline-text-3" id="text-org702c52e">
<p>
I will most likely update again in week 17 (when the report is due) since I&rsquo;d
rather spend more time writing report that this log. It&rsquo;s also tedious to
document minor report additions everywhere.
</p>
</div>
</div>
</div>

<div id="outline-container-org3a75f81" class="outline-2">
<h2 id="org3a75f81">Week 13</h2>
<div class="outline-text-2" id="text-org3a75f81">
<p>
We have made a lot of progress this week on all three components, that being
</p>
<ol class="org-ol">
<li>Audio generation</li>
<li>Structure generation</li>
<li>Combination of above</li>
</ol>
</div>
<div id="outline-container-orgc73161e" class="outline-3">
<h3 id="orgc73161e">Audio generation</h3>
<div class="outline-text-3" id="text-orgc73161e">
<p>
To start, here are some of the same note generated from piano notes over the
course of 513K steps.
</p>

<div class="columns">
<div class="column">
<p>
<b>44K</b>
</p>

<p>
 <audio controls="controls" src="audio/44k.wav"></audio>
</p>

</div>
<div class="column">
<p>
<b>62K</b>
</p>

<p>
 <audio controls="controls" src="audio/62k.wav"></audio>
</p>

</div>
<div class="column">
<p>
<b>83K</b>
</p>

<p>
 <audio controls="controls" src="audio/83k.wav"></audio>
</p>

</div>

</div>

<div class="columns">
<div class="column">
<p>
<b>153K</b>
</p>

<p>
 <audio controls="controls" src="audio/153k.wav"></audio>
</p>

</div>
<div class="column">
<p>
<b>230K</b>
</p>

<p>
 <audio controls="controls" src="audio/230k.wav"></audio>
</p>

</div>
<div class="column">
<p>
<b>513K</b>
</p>

<p>
 <audio controls="controls" src="audio/513k.wav"></audio>
</p>

</div>

</div>

<p>
They sound pretty good, but still very artificial. These are from spectrograms
of size \(128 \times 256\).
</p>
</div>

<div id="outline-container-org718c31e" class="outline-4">
<h4 id="org718c31e">Pitch conditioning</h4>
<div class="outline-text-4" id="text-org718c31e">
<p>
Due to our audio results not being good enough, we experimented with a deeper
model with more convolution layers. This ended up not being that easy to train,
and had trouble learning anything at all.
</p>

<p>
The shallow model used last week had a lot of trouble with classifying pitches.
The primary reason we think is the resolution being too low. To demonstrate
this, we have increased the resolution of spectrograms to \(128 \times 256\), which
doubles the number of frequency &rsquo;bins&rsquo;.
</p>


<div id="orge98ea11" class="figure">
<p><img src="./img/high_res_pitch.png" alt="high_res_pitch.png" />
</p>
<p><span class="figure-number">Figure 1: </span>A mel spectrogram of every pitch available in the gansynth dataset. These are generated pitches from training our shallow model, so they are not perfect but they demonstrate how resolution matters. The label number indicates MIDI pitch. Note how a higher pitch decreases space between overtones.</p>
</div>

<p>
The spectrograms in figure <a href="#orge98ea11">1</a> are generated using our shallow
model and the higher resolution. As we have already explained, the horizontal
lines are overtones. Between a tone and an overtone, there is a finite amount of
space that has to be large enough for 11 other notes, those being the other
musical notes (12 tones in total).A larger resolution ensures these 11 notes fit
into the spectrogram bins, which is vital for the auxiliary classifier to
recognise them.
</p>

<p>
The second, somewhat larger issue is that the space between overtones change
depending on the pitch of note. The nth overtone frequency is calculated as
\[f_{n} = n \times f_{0}\] where \(f_{0}\) is the base frequency of the note (or
pitch). As such, a very low pitch has very little space between the overtones
compared to a high note. Even though the spectrogram uses logarithmic scale on the
y-axis, it only counteracts this so that large pitches stay within small images.
It does not expand already low frequencies. The generated examples in
<a href="#orge98ea11">1</a> demonstrate this by failing completely to generate very low
frequency notes.
</p>

<p>
A third issue form these results are that some single pitches also fail to
generate. Our theory is that some pitches fall into the same bin, causing the
classifier to have to pick between two images that look like they have the same
pitch, but don&rsquo;t. A higher resolution should also solve this if that is the case.
</p>

<p>
For our \(128 \times 128\), the classifier the generator was never able to
generate the correct pitches, never deceasing its loss. For the above
\(128 \times 256\), it can somehow manage, but not enough. At the time of writing,
this model has trained for 107k steps, reaching great sounding results for some
of the notes much faster than any other attempt. Here are some examples sampled
from the middle of the pitch interval.
</p>

<div class="column">

<div id="org16fa9f2" class="figure">
<p><img src="./img/sample_pitches.png" alt="sample_pitches.png" />
</p>
<p><span class="figure-number">Figure 2: </span>Sample pitches 42, 43, 44, 45</p>
</div>

<p>
 <audio controls="controls" src="audio/sample_pitches.wav"></audio>
</p>

</div>

<p>
Of course, they don&rsquo;t sound perfect yet at such low period of training, but they
demonstrate pitch classification and a somewhat good piano sound. We found an
even higher resolution to both be too demanding for the computer, but also that
our sample rate is too low for it to correctly fill all information in the
spectrogram. This is visualized in figure <a href="#org3e42d26">3</a>
</p>


<div id="org3e42d26" class="figure">
<p><img src="./img/512.png" alt="512.png" />
</p>
<p><span class="figure-number">Figure 3: </span>An example of a \(128 \times 512\) spectrogram. Note the very tiny horizontal bars at the bottom, indicating that the sample frequency is too low to fill those bins.</p>
</div>
</div>
</div>

<div id="outline-container-org7c502d1" class="outline-4">
<h4 id="org7c502d1">Constant Q transform</h4>
<div class="outline-text-4" id="text-org7c502d1">
<p>
As shown before, we have been having issues with some pitches not generating
particularly well. The theory is that the resolution is not high enough for the
model to differentiate between two pitches. To solve this problem, we have to
delve deep into signal processing. Apologies if the following section is not
very explanatory, we have not had a course in this.
</p>

<p>
Right now, we use a simple fast fourier transform to generate our mel
spectrograms. This is fast and easy to do, but keeps resolution constant across
all frequencies. We visualised this last week, and there stated that we had
trouble particularly with the lower frequencies having enough space. The
<code>Constant Q transform</code> somewhat solves this by letting the window length be a
multiple of the previous window. The window then grows with each bin, making it
larger for higher frequencies and smaller for lower. This means the
transform works better for our hearing, which follows a log scale.
</p>

<p>
To use this, we simply switch which function to use. Figure <a href="#orgb4c16d2">4</a>
illustrates some results after a short training period of 60K steps.
</p>


<div id="orgb4c16d2" class="figure">
<p><img src="./img/cqt_exp_1.png" alt="cqt_exp_1.png" />
</p>
<p><span class="figure-number">Figure 4: </span>CQT after around 60K steps. These are the usual notes sampled from the middle but because lower frequencies have more space, the appear further up in the spectrogram.</p>
</div>

<p>
Currently, not quite all notes fit in the spectrogram, we might be able to
resolve this by changing the parameters a bit but for now, these show good results.
</p>

<p>
It should be noted that this transform is significantly slower due to not being
as easy to optimise as a standard fast fourier transform. Fortunately, we have
cached the entire dataset by the first epoch, so we see minimal performance hit
for this.
</p>
</div>
</div>

<div id="outline-container-orgc41851c" class="outline-4">
<h4 id="orgc41851c">Phase channel and instantaneous frequency</h4>
<div class="outline-text-4" id="text-orgc41851c">
<p>
One issue we have had for a while now is the Griffin-Lim algorithm for
rebuilding phase. Spectrograms include only magnitude of the fourier transform
and discards the phase entirely. This means inverting requires us to rebuild the
phase, which is what the algorithm does. It is a slow algorithm, requiring
multiple iterations of transforms to get it right. Additionally, it is not a
perfect replica, so the result sounds artificial.
</p>

<p>
The issues with this algorithms are
</p>
<ol class="org-ol">
<li>Too slow for our product</li>
<li>Mediocre results</li>
</ol>

<p>
The following is not an original idea and first used by gansynth. One way to
solve both these issues is not having to rebuild the phase. Instead, we let the
network generate it. This sounds more difficult than it actually is. All we have
to do is double the channels models into (magnitude, phase), and ensure the
dataset is processed to not throw away the phase. Recreating the transformed
signal is as easy as \(\text{mag} \times \text{phase}\) as you would using
phasors.
</p>

<p>
The more difficult problem is how to get the network to correctly learn phase.
It is easy for the network to learn magnitude because it is simply mapping the
color of a pixel in the image and activating it whenever it learnt to. Phase is
not constant for audio, the same pitch and note can have different phases
depending on the actual audio recording. This means just feeding the network
pitch would force it to map an infinite number of phases to our pitches, which
is not very easy for it to learn. This is where instantaneous frequency comes
in, which we have maybe mentioned before in this log. Basically, just take the
derivative of the phase, which remains constant. Then, the second channel
described earlier is just another image channel for the GAN to learn, no more
difficult than learning black and white images vs color.
</p>

<p>
Currently, we are training a network to generate these two-channel images using
Constant Q transform. We will probably have some results by next week.
</p>
</div>
</div>
</div>

<div id="outline-container-org9bb589f" class="outline-3">
<h3 id="org9bb589f">Structure generation</h3>
<div class="outline-text-3" id="text-org9bb589f">
<p>
Mostly just training it and debugging so that it doesn&rsquo;t copy its primer.
Training takes a long time, so we are also developing tools to analyse how it is
doing.
</p>
</div>
</div>
<div id="outline-container-orgf2a1c98" class="outline-3">
<h3 id="orgf2a1c98">Combining models</h3>
<div class="outline-text-3" id="text-orgf2a1c98">
<p>
We started work on combining the melody (transformer) and pitch (GAN) networks,
with some success. It works, though as we did not have any trained transformer,
the melodies are not great. The pitches are rather turbid, but we hope this will
improve with the Constant Q transform. It is also quite slow (converting
a manually chosen three minute MIDI file took half an hour), but this will also
improve with the Constant Q transform and phase channel.
</p>

<div class="columns">
<div class="column">
<p>
 <audio controls="controls" src="audio/gen_melody_no_train.wav"></audio>
</p>

<p>
A melody generated by an untrained transformer, performed by a GAN
</p>

</div>

<div class="column">
<p>
 <audio controls="controls" src="audio/gen_melody_real_midi.wav"></audio>
</p>

<p>
A melody performed by the GAN using real MIDI to better show that part
</p>

</div>

</div>
</div>
</div>
<div id="outline-container-org24dca14" class="outline-3">
<h3 id="org24dca14">Report</h3>
<div class="outline-text-3" id="text-org24dca14">
<p>
We have started working in an agile fashion, using sprints to ensure we merge
our changes into the report. This means defining small tasks that have to be
completed by next sprint planning session, which occurs each week. This should
make it easier for everyone to contribute to the report and know what everyone
is working on.
</p>

<p>
We also had a meeting with our supervisor where we got a lot of great feedback
on things to change in the report. We hope to do those by next Wednesday.
</p>
</div>
</div>
<div id="outline-container-org03c3695" class="outline-3">
<h3 id="org03c3695">Summary of each member</h3>
<div class="outline-text-3" id="text-org03c3695">
<ul class="org-ul">
<li><b>Christoffer</b>:
<ul class="org-ul">
<li>Been focusing entirely on the note generation.</li>
<li>Setup model for higher resolution, 128x256 works well, 128x512 is not possible due to too low sample rate</li>
<li>A lot of research regarding signal processing transforms and their limitations so that we can correctly capture the full range of notes,</li>
<li>Experimented with using constant Q transform rather than stft in order to widen low frequency resolution.</li>
<li>Started training using Constant Q transform, added phase channel with the
help of the entire group debugging my code.</li>
<li>Documented all that in the log, as well as wrote about ACGAN in the report.</li>
</ul></li>
<li><p>
<b>Eric</b>:
</p>
<ul class="org-ul">
<li>Helped with combining models</li>
<li>Training the transformer and debugging why it copies prior (the prior and target overlapped).</li>
<li>Wrote on the transformer section in implementation.</li>
<li>Reviewed some pull requests for the report.</li>
</ul>
<p>
-Read through and comment on the report for our weekly sprint.
</p></li>
<li><b>Carl</b>:
<ul class="org-ul">
<li>Combining models</li>
<li>Lots of implementation help all over the place</li>
<li>Pull requests and reviews</li>
</ul></li>
<li><b>Lovisa</b>:</li>
<li><b>Cao</b>:
<ul class="org-ul">
<li>Writing introduction for machine learning section, solving some issues on github.</li>
</ul></li>
<li><b>Elias</b>:
<ul class="org-ul">
<li>Read through the entire report and tried to comment on everything from style/structure to spelling mistakes.</li>
<li>Experimented with a new vae gan model</li>
<li>Connected the gan and transformer models together with Carl, Lovisa, and Eric.</li>
<li>Implemented Constant Q transform and phase channel with Christoffer.</li>
<li>Reviewed and merged PRs for the report.</li>
<li>Planned next report writing sprint.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgc0a5153" class="outline-3">
<h3 id="orgc0a5153">Last weeks&rsquo; goals</h3>
<div class="outline-text-3" id="text-orgc0a5153">
<ul class="org-ul">
<li>Went way ahead of last weeks goals regarding pitch conditioning</li>
<li>We finished connecting structure and audio generation (in a simple way but still)</li>
</ul>
</div>
</div>
<div id="outline-container-org79b1f29" class="outline-3">
<h3 id="org79b1f29">Next week</h3>
<div class="outline-text-3" id="text-org79b1f29">
<ul class="org-ul">
<li>Probably mostly report writing, we got some good feedback from our supervisor
meeting this week.</li>
<li>Fine-tune all our components and wait while training them.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org19f350e" class="outline-2">
<h2 id="org19f350e">Week 12</h2>
<div class="outline-text-2" id="text-org19f350e">
<p>
We had to cancel both the supervisor meeting and the Friday meeting with our
group that we hold each week, so this week may not correctly reflect all
progress that have happened in the project. I (Christoffer) try to sum
everything up but it will always focus more on what I&rsquo;ve been doing due to me
writing the log. Please refer to the time log for more specific work by other members.
</p>
</div>

<div id="outline-container-orga462daf" class="outline-3">
<h3 id="orga462daf">Progress</h3>
<div class="outline-text-3" id="text-orga462daf">
<p>
We are currently working hard on getting pitch conditioning to work since that
is the bottleneck for connecting with structure. To accomplish this, we are
constantly reading about ways of conditioning GANs, one of which is using an
<i>auxiliary classifier</i>.
</p>
</div>

<div id="outline-container-orgcf528dd" class="outline-4">
<h4 id="orgcf528dd">Pitch conditioning and Auxiliary Classifier</h4>
<div class="outline-text-4" id="text-orgcf528dd">
<p>
As explained last week, we require this step to be able to choose which notes
are being played. However, from our research and from limited testing, we found
that the generator rarely if ever made use of the additional pitch information.
To encourage the generator to actually use it, we implement an auxiliary
classifier loss to the network.
</p>


<div id="orgb826c83" class="figure">
<p><img src="./img/aux_class_overview.png" alt="aux_class_overview.png" />
</p>
<p><span class="figure-number">Figure 5: </span>Overview of how a Auxiliary Classifier GAN works (ACGAN)</p>
</div>

<p>
The <i>ACGAN</i> in figure <a href="#orgb826c83">5</a> shows pitch class as \(c\), noise input as \(z\),
and the real image as x (which also needs a pitch class)
</p>

<p>
Without explaining exactly how a GAN works, there are two networks, a generator
and a discriminator. The aux classifier comes in at the discriminator, who
normally only has the job of determining fake vs real images from its input
(either an image from the dataset or form the generator). With an aux
classifier, there is a second loss, depending on if the discriminator can
correctly label the pitch of the generated image.
</p>

<p>
The discriminators \(D\) job is then to maximise \[L_{D} = L_{C} + L_{S}\] where
\(L_{C}\) is the log-likelihood of correct class (or pitch in this case), and
\(L_{S}\) is the log-likelihood of correct source (real vs fake image).
</p>

<p>
The generators \(G\) job is then to maximise \[L_{G} = L_{C} - L_{S}\] or in
words, get D to determine the class label right, but fail at determining real vs
fake.
</p>

<p>
With an actual loss to the discriminator and generator, the hope is they won&rsquo;t
ignore the pitch vectors anymore. We have almost implemented all this, but are
still experimenting with getting it entirely correct.
</p>
</div>
</div>

<div id="outline-container-org5e6e929" class="outline-4">
<h4 id="org5e6e929">Training</h4>
<div class="outline-text-4" id="text-org5e6e929">
<p>
Our required time to train has gone up drastically. For example, the pitch
conditioning with an auxiliary classifier (but probably without one as well)
required 2.5 days to train. That is around 1.5M steps. We may have been able
to decrease this but that is still a lot of time spent on waiting for results
that may or may not be great. This is after we decreased each epoch (~200 steps)
from 40 seconds to 11 seconds by caching all images from the dataset into memory.
</p>

<div class="columns">
<div class="column">

<div id="org5b2e4a5" class="figure">
<p><object type="image/svg+xml" data="./img/embedding_gen_loss.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
<p><span class="figure-number">Figure 6: </span>Generator loss when training with an embedding</p>
</div>

</div>

<div class="column">

<div id="org8cc59f9" class="figure">
<p><object type="image/svg+xml" data="./img/embedding_disc_loss.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
<p><span class="figure-number">Figure 7: </span>Discriminator loss when training with an embedding.</p>
</div>

</div>

</div>


<p>
Figures <a href="#org5b2e4a5">6</a> and <a href="#org8cc59f9">7</a> shows that it does reach an
equilibrium, which is where the training is optimal. It does reach this stage,
but determining when to stop is a difficult thing. Another issue that arises
with longer training periods is over-fitting on the dataset, which is not idea
either. The skips in the figure are due to us requiring to stop the training to
use the GPUs to generate results.
</p>
</div>
</div>

<div id="outline-container-orgd58ac58" class="outline-4">
<h4 id="orgd58ac58">Current Results of Note Generation with Conditioning and Classifier</h4>
<div class="outline-text-4" id="text-orgd58ac58">
<p>
The following results were taken from the above training session at the last
skip and at the end. (steps 600K and 1600K). The pitches used are increasing,
but due to a bug in our code, the spectrograms are flipped on the y axis (as the
plot suggests). Here, we use an embedding of the pitch label instead of a one-hot
encoding.
</p>

<div class="columns">
<div class="column">

<div id="orgd956bd3" class="figure">
<p><img src="./img/600k_steps.png" alt="600k_steps.png" />
</p>
<p><span class="figure-number">Figure 8: </span>Result at 600K steps</p>
</div>

<p>
 <audio controls="controls" src="audio/600k_steps.wav"></audio>
</p>

</div>

<div class="column">

<div id="orgbed0de9" class="figure">
<p><img src="./img/1600k_steps.png" alt="1600k_steps.png" />
</p>
<p><span class="figure-number">Figure 9: </span>Result at 1600K steps</p>
</div>

<p>
 <audio controls="controls" src="audio/1600k_steps.wav"></audio>
</p>

</div>

</div>


<p>
The main issue in this seems to be mode collapse, the generator no matter pitch
or noise input generates the same or similar examples. In the case of figure
<a href="#orgbed0de9">9</a>, there are three kinds of images, of which only one of them
resembles a keyboard (which is what we are training on). Arguably, you could
have noticed mode collapse in figure <a href="#orgd956bd3">8</a>, but we thought we would
give it a try anyway.
</p>

<p>
The next experiment we ran was a deeper network with much more trainable
parameters. We trained it overnight on only acoustic keyboard sounds from the
GANSynth subset of NSynth.
</p>

<div class="columns">
<div class="column">

<div id="org4d59b3f" class="figure">
<p><object type="image/svg+xml" data="./img/deep_gen_loss.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
<p><span class="figure-number">Figure 10: </span>Generator loss from a deeper network</p>
</div>

</div>

<div class="column">

<div id="org9de290d" class="figure">
<p><object type="image/svg+xml" data="./img/deep_disc_loss.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
<p><span class="figure-number">Figure 11: </span>Discriminator loss from a deeper network</p>
</div>

</div>

</div>

<p>
Figures <a href="#org4d59b3f">10</a> and <a href="#org9de290d">11</a> show an unstable network
Ideally, they converge to the Nash equilibrium and stay around there. Due to
this issue and that the deeper model requires more time to train to see
improvement, we reverted back to our previous, shallower networks.
</p>

<p>
Next experiment was using one-hot encoding of pitch with a shallower network
(quicker to train). We also multiplied the aux component of the losses by a
weight of 10, hoping it would prioritise decreasing that penalty. For this
experiment, we also added logging images in tensorboard so we didn&rsquo;t have to
stop training while seeing model progress.
</p>

<div class="columns">
<div class="column">

<div id="org83d3f82" class="figure">
<p><object type="image/svg+xml" data="./img/shallow_gen_loss.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
<p><span class="figure-number">Figure 12: </span>Generator loss from the shallower network, with one-hot encoding and a weighted aux loss.</p>
</div>

</div>

<div class="column">

<div id="org366a644" class="figure">
<p><object type="image/svg+xml" data="./img/shallow_disc_loss.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
<p><span class="figure-number">Figure 13: </span>Discriminator loss from the shallower network, with one-hot encoding and a weighted aux loss.</p>
</div>

</div>

</div>

<div class="columns">
<div class="column">

<div id="orgdb5d4b1" class="figure">
<p><img src="./img/shallow_spec_early.png" alt="shallow_spec_early.png" />
</p>
<p><span class="figure-number">Figure 14: </span>Early images at step count ~30K</p>
</div>

</div>
<div class="column">

<div id="org033c9df" class="figure">
<p><img src="./img/shallow_spec_mid.png" alt="shallow_spec_mid.png" />
</p>
<p><span class="figure-number">Figure 15: </span>Early images at step count ~50K</p>
</div>

</div>
<div class="column">

<div id="org434ca8f" class="figure">
<p><img src="./img/shallow_spec_late.png" alt="shallow_spec_late.png" />
</p>
<p><span class="figure-number">Figure 16: </span>Later images at step count ~200K</p>
</div>

</div>

</div>

<p>
The loss graphs in figures <a href="#org83d3f82">12</a> and <a href="#org366a644">13</a> look
much better than before. Not visualised in them is that the discriminator starts
from ~10, while the generator stars from ~40. This is because of the weighted
aux loss described earlier. Figure <a href="#orgdb5d4b1">14</a> shows to us almost
random noise but at this point almost all loss from the aux is eliminated. At
around step ~40K the generator starts generating images that resembles
spectrograms, shown in figure <a href="#org033c9df">15</a>. Figure <a href="#org434ca8f">16</a>
shows accurate spectrograms representations.
</p>

<p>
Although we have generated pitches in increasing fashion for these figures, the
generator does not map them in that fashion. To show that pitch conditioning
does indeed work, we provide the generator different noise inputs \(z\) but the same
pitch classes \(c\).
</p>

<div class="column">

<div id="org74bb7a5" class="figure">
<p><img src="./img/same_pitch_spec.png" alt="same_pitch_spec.png" />
</p>
<p><span class="figure-number">Figure 17: </span>Spectrograms generated with different noise vectors and the same pitch class.</p>
</div>

<p>
 <audio controls="controls" src="audio/same_pitch_spec.wav"></audio>
</p>

</div>

<p>
Note that while figure <a href="#org74bb7a5">17</a> may look like mode collapse just like
before, they look similar due to us choosing to generate only a specific pitch.
The horizontal lines that make up the note are similar due to this, but timbre
which is represented by everything else in the image differs.
</p>

<p>
With this, we can select which pitch to generate, but not reliably. Our use case
requires us to be able to select for instance the note A, but here we have no
idea which one-hot vector represents that. Why it didn&rsquo;t learn them in order, we
don&rsquo;t know. But solving this problem will be our next priority.
</p>

<p>
Also, some processing was wrong with this generation that affects how
normalising spectrograms work. Normalising is not a simple operation, and
depends on the datasets standard deviation and variance. This requires us to
process the entire dataset before training to calculate these. Because we
switched to only guitars, the old one calculated is not correct, and we forgot
to recalculate it.
</p>

<p>
This variant will continue training until we are sure it won&rsquo;t improve more, but
for now, these are the results we have.
</p>
</div>
</div>

<div id="outline-container-orgbdd7e87" class="outline-4">
<h4 id="orgbdd7e87">Pitch experiments</h4>
<div class="outline-text-4" id="text-orgbdd7e87">
<p>
This section is just an addition of some pitch experiments we performed to try
to figure out if the classifier works correctly. Not a lot of progress here but
it is interesting noting things down.
</p>

<p>
As of writing this, the model has trained for 800k steps. At one point, it
introduced a lot of extra noise but quickly eliminated it.
</p>

<div class="columns">
<div class="column">

<div id="orge03bbc0" class="figure">
<p><object type="image/svg+xml" data="./img/800k_gen_loss.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
<p><span class="figure-number">Figure 18: </span>generator loss from training at 800k steps</p>
</div>

</div>

<div class="column">

<div id="orgfb8fd3d" class="figure">
<p><object type="image/svg+xml" data="./img/800k_disc_loss.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
<p><span class="figure-number">Figure 19: </span>discriminator loss from training at 800k steps</p>
</div>

</div>

</div>

<div class="column">

<div id="org3266b69" class="figure">
<p><img src="./img/pitch_exp_64.png" alt="pitch_exp_64.png" />
</p>
<p><span class="figure-number">Figure 20: </span>64 different pitches generated with the same seed. the last 3 (61,62,63) do not exist so they look like junk. there is no discernible order or pattern in this set but the pitches generated do seem unique and changes depending on pitch class.</p>
</div>

<p>
 <audio controls="controls" src="audio/pitch_exp_64.wav"></audio>
</p>

</div>

<p>
Figure <a href="#org3266b69">20</a> shows that pitch class matters and changes the item
generated by a noticable amount, but that they don&rsquo;t appear to be sorted like
the pitches in the dataset are though it is a bit difficult to tell though it is
a bit difficult to tell.
</p>
</div>
</div>

<div id="outline-container-org9348fab" class="outline-4">
<h4 id="org9348fab">Report</h4>
<div class="outline-text-4" id="text-org9348fab">
<p>
While we spent a great deal focusing this week on having a product to show,
we&rsquo;ve made a bit of progress on the report, specifically the introduction,
theory and implementation portions.
</p>

<ul class="org-ul">
<li><b>Introduction</b>: Drafted the background.</li>
<li><b>Theory</b>: We improved our explanations of things, mostly the transformer.</li>
<li><b>Implementation</b>: Start describing all the various parts, many that we worked on
last week like conditioning. Also worked on the transformer portion.</li>
</ul>

<p>
We also made figures that helps describe some of the complex things
(particularly the transformer). We think they are preferable over text.
</p>
</div>
</div>
</div>

<div id="outline-container-org67eebee" class="outline-3">
<h3 id="org67eebee">Last weeks&rsquo; goals</h3>
<div class="outline-text-3" id="text-org67eebee">
<ul class="org-ul">
<li>A lot of progress on auxiliary classifier.</li>
<li>Holding off on incorporating WGAN.</li>
<li>Connecting everything was not possible quite yet, but maybe next week.</li>
<li>Implementation of report improved.</li>
</ul>
</div>
</div>
<div id="outline-container-org10f215c" class="outline-3">
<h3 id="org10f215c">Summary of each member</h3>
<div class="outline-text-3" id="text-org10f215c">
<p>
This week was Easter, so many group members were busy elsewhere.
</p>
<ul class="org-ul">
<li><b>Christoffer</b>:
<ul class="org-ul">
<li>Mostly worked on implementing and training the ACGAN.</li>
<li>Also been writing the implementation chapter and the log in the dead time
between training.</li>
</ul></li>
<li><b>Eric</b>:</li>
<li><b>Carl</b>: Created tikz figures of RNN and seq2seq used in theory</li>
<li><b>Lovisa</b>:</li>
<li><b>Cao</b>: I did a bit of pull request review and final report but mostly was sick</li>
<li><b>Elias</b>: Worked with Christoffer on ACGAN. Minor work on the report.</li>
</ul>
</div>
</div>
<div id="outline-container-org255ccb6" class="outline-3">
<h3 id="org255ccb6">Next week</h3>
<div class="outline-text-3" id="text-org255ccb6">
<ul class="org-ul">
<li>Fine tune conditioning and figure out a way of reliably choosing which pitch
it generates. Also try improve audio quality so it sounds like more than just bells.</li>
<li>Start connecting structure and note gen. This should be possible now that we
can at least try choosing pitches, even if they are not quite correctly mapped yet.</li>
<li>As usual, write the report. We want to start making the text cohesive; it&rsquo;s
quite jumbled right now.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org6a9314c" class="outline-2">
<h2 id="org6a9314c">Week 11</h2>
<div class="outline-text-2" id="text-org6a9314c">
<p>
No great audio results for this week unfortunately.
</p>
</div>
<div id="outline-container-org69520c4" class="outline-3">
<h3 id="org69520c4">Progress</h3>
<div class="outline-text-3" id="text-org69520c4">
</div>
<div id="outline-container-org01dfbc1" class="outline-4">
<h4 id="org01dfbc1">Report</h4>
<div class="outline-text-4" id="text-org01dfbc1">
<p>
We have made good progress on the report, though we feel that since our
implementation is so advanced compared to the typical bachelor thesis, the
theory section has to be really large. As such, we have primarily been writing
theory and a bit on implementation.
</p>

<p>
We came up with a good structure with our supervisor and are right now focusing
on making each chapter have roughly the same amount of pages. We could easily
write too much for this report, so we have to find a balance of what is actually
required to explain to understand the implementation. An example is: how do you
explain how a transformer with attention work? This topic could easily take ~20
pages but leaving it too short would make understanding difficult for the reader
who we target as our course mates.
</p>
</div>
</div>

<div id="outline-container-org3db2563" class="outline-4">
<h4 id="org3db2563">Note generation</h4>
<div class="outline-text-4" id="text-org3db2563">
<p>
In order to connect this component to the final system, we need to condition it
on pitch so we can controlling which pitch/note the generator generates. This is
what we have been working on this week. The main idea is to append a one-hot
vector representation of notes (size of 88 which are the number of keys on a
standard piano or number of tones in MIDI). The hope is that the generator then
learns to generate random sounding tones according to that vector.
</p>

<p>
Pitch conditioning is almost done but we cannot be sure it is entirely right.
We may have to add another loss component to the discriminator called an
<i>auxiliary classifier</i> that would encourage the generator to care more about the
conditioning vector.
</p>

<p>
This idea came from the GANSynth paper. They do a lot of other things as well,
including adding a phase channel to the network, allowing it to generate its own
phase instead of having to reconstruct it using the Griffin-lim algorithm. They
call this Instantaneous Frequency, which is the derivative of phase. We also
want to implement this because even though our processing and inversion sounds
&ldquo;good enough&rdquo;, this could improve it.
</p>

<p>
Due to limited training, we do not have a lot of great sounding examples yet so
but hopefully we will have some by next week.
</p>
</div>
</div>

<div id="outline-container-orgcf29d37" class="outline-4">
<h4 id="orgcf29d37">Structure generation</h4>
<div class="outline-text-4" id="text-orgcf29d37">
<p>
Structure generation is since last week pretty much done, maybe we could train
it a bit more but our main concern is fixing note generation so that we can
connect the two.
</p>
</div>
</div>

<div id="outline-container-orgced427b" class="outline-4">
<h4 id="orgced427b">Training</h4>
<div class="outline-text-4" id="text-orgced427b">
<p>
We started using Bayes but we are not quite sure if we are using it correctly.
The canvas course page states to ensure there are not more threads running than
designated CPU cores, so the main concern is configuring TensorFlow to not spawn
~200 threads.
</p>

<p>
We also set up tensorboard to better log our results. We will most likely start
auto generating our images and audio samples but it&rsquo;s not a priority.
</p>
</div>
</div>
</div>

<div id="outline-container-orga315e94" class="outline-3">
<h3 id="orga315e94">Last weeks&rsquo; goals</h3>
<div class="outline-text-3" id="text-orga315e94">
<ul class="org-ul">
<li>We did some extensive training on Bayes</li>
<li>Spectrogram processing is for the most part done (there are still some issues
with normalisation)</li>
<li>Wasserstein not implemented yet due to wanting to focus on processing, who we
deemed to be the thing making the note generation sound off.</li>
<li>Posterior collapse still an issue</li>
<li>We&rsquo;ve come a good way with the report but there is a lot of theory</li>
</ul>
</div>
</div>
<div id="outline-container-orgece8d75" class="outline-3">
<h3 id="orgece8d75">Summary of each member</h3>
<div class="outline-text-3" id="text-orgece8d75">
<ul class="org-ul">
<li><b>Christoffer</b>:
<ul class="org-ul">
<li>Spent most of my time structuring and writing the report.</li>
<li>Also worked with Eric and Elias to implement pitch conditioning on the
GAN.</li>
<li>Also helped debug our spectrogram processing though most of that work was
Eric.</li>
<li>Been really digging into the GANSynth details, even though they don&rsquo;t do
a great job of explaining their model.</li>
<li>Finally, did some training on Bayes and attempted to configure
TensorFlow/numpy to only use x amount of threads.</li>
</ul></li>
<li><b>Eric</b>:
<ul class="org-ul">
<li>I mostly worked on the VAE(which is starting to look like a dead-end to
me)</li>
<li>Worked on GAN to add the pitch conditioning and fix normalisation.</li>
</ul></li>
<li><b>Carl</b>:
<ul class="org-ul">
<li>Wrote on a few sections including ethics and music transformer, rewrote
midi pipeline</li>
</ul></li>
<li><b>Lovisa</b>:
<ul class="org-ul">
<li>Working on Wasserstein GAN and the maths behind it.</li>
<li>Have been reviewing GitHub issues and added content to the report.</li>
</ul></li>
<li><b>Cao</b>:
<ul class="org-ul">
<li>Did some reading and expanded some of the subsections for the final
report.</li>
<li>Also reviewed the specgan/ pipeline for more understanding so id be able
to help with writing the implementation section/ documentation for the
code.</li>
</ul></li>
<li><b>Elias</b>:
<ul class="org-ul">
<li>Worked on getting pitch conditioning to work together with Christoffer and
Eric.</li>
<li>Also worked on implemented an alternative GAN training scheme which
has some similarities to Wasserstein GAN.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org0cde2ee" class="outline-3">
<h3 id="org0cde2ee">Next week</h3>
<div class="outline-text-3" id="text-org0cde2ee">
<ul class="org-ul">
<li>Auxiliary classifier for the SpecGAN</li>
<li>Maybe incorporate WGAN if results are not great</li>
<li>Work on connecting structure and note, even if note is not on par with
structure yet.</li>
<li>Keep adding to the report, particularly the implementation sections.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org5eefd09" class="outline-2">
<h2 id="org5eefd09">Week 10</h2>
<div class="outline-text-2" id="text-org5eefd09">
</div>
<div id="outline-container-orgbf21f6e" class="outline-3">
<h3 id="orgbf21f6e">Progress</h3>
<div class="outline-text-3" id="text-orgbf21f6e">
</div>
<div id="outline-container-org081c9f6" class="outline-4">
<h4 id="org081c9f6">Report</h4>
<div class="outline-text-4" id="text-org081c9f6">
<p>
More and more chapters are being handled, specifically the theory parts and
techniques we have used over the course of the project. There is also a complete
structure that makes it easy to add content. We have also been thinking about
how we present our journey in the report, and decided that we would add an
experiments section.
</p>
</div>
</div>
<div id="outline-container-org4562200" class="outline-4">
<h4 id="org4562200">Note Generation</h4>
<div class="outline-text-4" id="text-org4562200">
<p>
We are currently trying to implement a Wasserstein GAN, which should improve the
results of the GAN. Below are some audio files that have been processed back
from spectrograms generated by our model.
</p>

<p>
Generated results after training on the GANSynth dataset (slight changes from the NSynth
dataset) . <b>Warning, these are quite loud!</b>
</p>

<p>
 <audio controls="controls" src="audio/our_inverted_specs_fixed.wav"></audio>
</p>

<p>
They do not sound that good but some tonality is there. We think there may be a
problem with the inversion back into audio. To demonstrate this, the following
audio snippet is from real notes inverted into spectrograms and then back with
the same data pipeline:
</p>

<p>
 <audio controls="controls" src="audio/actual_inverted_spec_fixed.wav"></audio>
</p>

<p>
There are similar results indicating that there is an issue. Whether this is
due to errors in spectrogram generation or from inverting back, we have not
concluded yet.
</p>

<p>
A real inversion (with all parameters set properly) will not sound perfect due
to processing audio into a spectrogram throws away the phase information of the
signal. There are algorithms for rebuilding this (Griffith-lim) but they are not
perfect. Early tests in the project showed that the quality is good enough, much
better than the results we get now.
</p>
</div>
</div>

<div id="outline-container-orge358210" class="outline-4">
<h4 id="orge358210">Structure Generation</h4>
<div class="outline-text-4" id="text-orge358210">
<p>
Last week, the results sounded quite good. That is because the prior was very
long, the model ended up copying too much resulting in an existing song (with
some minor alterations). We think it may be due to overfitting on the dataset
(MAESTRO). The following audio snippet was generated with a smaller prior:
</p>

<p>
 <audio controls="controls" src="audio/mutrans_half_prior_half_gen.wav"></audio>
</p>

<p>
We believe the MIDI encoding is fine (good enough) since encoding and decoding a
real song gives good results (besides it not being lossless).
</p>
</div>
</div>

<div id="outline-container-orgaaf1e63" class="outline-4">
<h4 id="orgaaf1e63">Bayes Training</h4>
<div class="outline-text-4" id="text-orgaaf1e63">
<p>
We finally started training on Bayes; the first round is currently scheduled to
train 50 epochs of our VAE implementation. We do not yet have any results of this.
</p>
</div>
</div>
</div>
<div id="outline-container-org7ddc5b3" class="outline-3">
<h3 id="org7ddc5b3">Summary of each member</h3>
<div class="outline-text-3" id="text-org7ddc5b3">
<ul class="org-ul">
<li><b>Christoffer</b>:
<ul class="org-ul">
<li>Wrote initial draft for a datasets chapter, explaining the NSynth and MAESTRO datasets</li>
<li>Wrote an entire theory chapter on signal processing</li>
<li>Read up on how to use Bayes for scheduling running tasks</li>
<li>Review of pull requests and issues in our git report repo</li>
<li>Did some basic reading on Wasserstein GANs, but nothing major.</li>
<li>Looked into getting Tensorboard to work (required us to uninstall other versions of TensorFlow.</li>
</ul></li>
<li><b>Eric</b>:
<ul class="org-ul">
<li>Wrote on experiments section in report</li>
<li>Tried to figure out why the transformer is copying the prior with a lot of trial and error</li>
<li>Tried hyperparameter optimisation</li>
</ul></li>
<li><b>Carl</b>:
<ul class="org-ul">
<li>Some infrastructure and tech support</li>
<li>Work on report, mainly overall style and front pages, but also some content</li>
<li>Made some plots with pgfplots, but probably going to give that up</li>
</ul></li>
<li><b>Lovisa</b>:
<ul class="org-ul">
<li>Took over some of the email communications, have been writing to
Arne Linde about our computer at Chalmers and communicating with examinator
and supervisor too. Tried to be More active on git hub, reading comments and
making more reviews. Also kept working on wgan and the report.</li>
</ul></li>
<li><b>Cao</b>:
<ul class="org-ul">
<li>Read about RNN, LSTM, Transformer.</li>
<li>Made documentation for the transformer model.</li>
<li>Started writing subsections for the final report: What is ML and AI
models, Variational autoencoder and Deep neural networks.</li>
</ul></li>
<li><b>Elias</b>:
<ul class="org-ul">
<li>Added ability to generate samples of real and autoencoded audio samples
to the vae gan. Found that the results are very different so spent a lot
of time tweaking hyperparameters, modifying the training algorithm and
training to improve the results. Still no good results unfortunately.</li>
<li>Also wrote on the report. Specifically about transformers, wavenet, and
musenet.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org14d775d" class="outline-3">
<h3 id="org14d775d">Next week</h3>
<div class="outline-text-3" id="text-org14d775d">
<ul class="org-ul">
<li>More extensive training of models on Bayes</li>
<li>Fix spectrogram processing</li>
<li>Implement Wasserstein GAN (already in progress)</li>
<li>Fix posterior collapse in the VAE (similar to mode collapse in GANS)</li>
<li>Keep adding content to report</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgeefca24" class="outline-2">
<h2 id="orgeefca24">Week 9</h2>
<div class="outline-text-2" id="text-orgeefca24">
</div>
<div id="outline-container-orga6dd3f7" class="outline-3">
<h3 id="orga6dd3f7">Progress</h3>
<div class="outline-text-3" id="text-orga6dd3f7">
</div>
<div id="outline-container-orgdbc0adf" class="outline-4">
<h4 id="orgdbc0adf">Report</h4>
<div class="outline-text-4" id="text-orgdbc0adf">
<p>
We created a detailed outline of sections in the report that will make it
easy to add things as we develop them. We&rsquo;ve also started writing parts of the
Theory chapter, explaining basic concepts. Finally, we want to write about the
models we&rsquo;ve implemented and tested.
</p>
</div>
</div>

<div id="outline-container-orgc00579a" class="outline-4">
<h4 id="orgc00579a">Shift of focus</h4>
<div class="outline-text-4" id="text-orgc00579a">
<p>
We decided to shift our focus entirely to the transformer and variational auto
encoder since we felt the wave2midi2wave wouldn&rsquo;t pan out in a way we would
hope.
</p>
</div>
</div>

<div id="outline-container-org8613abf" class="outline-4">
<h4 id="org8613abf">VAE</h4>
<div class="outline-text-4" id="text-org8613abf">
<p>
Elias and cao were assigned to this task, but since exams, not a lot of progress
in this area was made this week. The model has however had similar results to
SpecGan, so we are still researching this.
</p>
</div>
</div>

<div id="outline-container-orga6c697c" class="outline-4">
<h4 id="orga6c697c">Transformer</h4>
<div class="outline-text-4" id="text-orga6c697c">
<p>
The transformer aims to deal with the structure of music. It trains on MIDI,
learning the relationships between sequences of MIDI notes and outputs the most
appropriate notes. The aim of this model is to connect it with the note
generation to generate complete music.
</p>

<p>
We&rsquo;ve been working towards getting the music transformer model running and also
implementing our own version of it. This week, we managed to run both and
generate results. Below are the audio snippets of the two.
</p>

<p>
<b>Our music transformer with prior</b>
</p>

<p>
 <audio controls="controls" src="audio/mutrans_our_prior.wav"></audio>
</p>

<p>
 <audio controls="controls" src="audio/mutrans_our_prior2.wav"></audio>
</p>

<p>
<b>Music transformer implementation found online without prior</b>
</p>

<p>
 <audio controls="controls" src="audio/mutrans_test.wav"></audio>
</p>

<p>
As you can hear, all three examples have structure, which is promising. We
will continue working on these and later connecting it with our note generation!
</p>
</div>
</div>

<div id="outline-container-org8710a57" class="outline-4">
<h4 id="org8710a57">Training resources</h4>
<div class="outline-text-4" id="text-org8710a57">
<p>
We got access to another training platform, Bayes at DS&amp;AI division. This server
has much better hardware than the previous one, but also restrictions when it comes
to time slots to train and amount of training. We&rsquo;ll eventually use it to most
likely train the transformer since it requires better specs than we had.
</p>
</div>
</div>

<div id="outline-container-org22483cd" class="outline-4">
<h4 id="org22483cd">Meetings</h4>
<div class="outline-text-4" id="text-org22483cd">
<p>
Meetings has been going well online, we try to work more in voice calls and limit
how much we meet in person. It is still challenging ensuring everyone has tasks
to work on and ensuring everyone is on the same page.
</p>
</div>
</div>

<div id="outline-container-org97e0fbf" class="outline-4">
<h4 id="org97e0fbf">Exam week</h4>
<div class="outline-text-4" id="text-org97e0fbf">
<p>
Still exam week so some members haven&rsquo;t gotten a lot done
</p>
<ul class="org-ul">
<li><b>Christoffer</b>: Work on issues in the GitHub like structuring repo, Also
structured and started writing the report (signal processing in theory). A
lot of the time is writing scripts to generate plots we can use in the
report. Wrote guide on how to use training computer.</li>
<li><b>Eric</b>: Work on transformer, refactoring and general implementation details
(refactor project and split parts of code into separate runnable scripts).
Big issue for transformers is memory to train for long sequences and the
model copying its prior (initial input).</li>
<li><b>Carl</b>: Deploy script, refactoring and reviewing GitHub pull requests. Wrote
a progress bar module for our training scripts, showing progress of training.</li>
<li><b>Lovisa</b>: Skeleton/outline of report and also started on implementing
wasserstein loss for specgan(maths heavy so complete study mode of the
maths). Progress in understanding the subject so will add to report. Had
trouble setting up repo (our repo) on laptop, Carl helped with that.</li>
<li><b>Cao</b>: Set up things for the remote computer. Has been busy with other
courses to really participate much. Also as been missing an assigned task
which us in the group are working on fixing for next week.</li>
<li><b>Elias</b>: Not a lot of work since last meeting, mostly focused on other course
due to exam week.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org098e691" class="outline-3">
<h3 id="org098e691">Next week</h3>
<div class="outline-text-3" id="text-org098e691">
<ul class="org-ul">
<li>Keep writing report</li>
<li>Continue work on music transformer</li>
<li>More extensive training with training computer and potentially Bayes</li>
<li>Create more thorough tests (unit and integration) (from last week)</li>
<li>Write a bunch of utility functions (flags, plotting etc). (from last week)</li>
<li>Continue work on the VAE and maybe begin connecting everything</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orge821b23" class="outline-2">
<h2 id="orge821b23">Week 8</h2>
<div class="outline-text-2" id="text-orge821b23">
</div>
<div id="outline-container-org8eb0fb6" class="outline-3">
<h3 id="org8eb0fb6">Progress</h3>
<div class="outline-text-3" id="text-org8eb0fb6">
<p>
In general, because of the pandemic and exams, the project progressed less
than other weeks. There have been some progress with audio generation, but it
is hard to include audio snippets into this page so maybe they will exist in
our repo at some point.
</p>
</div>
<div id="outline-container-org63a413f" class="outline-4">
<h4 id="org63a413f">Training resources</h4>
<div class="outline-text-4" id="text-org63a413f">
<p>
We finally gained access to a computer we can use for training. This means a
lot of our time was spent on setup of this computer and porting of our colab
code to work on it.
</p>
</div>
</div>
<div id="outline-container-orgd3eeb1d" class="outline-4">
<h4 id="orgd3eeb1d">Meetings</h4>
<div class="outline-text-4" id="text-orgd3eeb1d">
<p>
Due to the pandemic, we may start holding meetings online rather than in
person (if multiple people message about not being able to join).
Supervision meetings are all held online for now on until further notice
from Chalmers.
</p>
</div>
</div>
<div id="outline-container-org8129d82" class="outline-4">
<h4 id="org8129d82">Exam week</h4>
<div class="outline-text-4" id="text-org8129d82">
<p>
Because it is exam time for other courses, a lot of group members had to
spend their time studying for those or writing reports.
</p>
</div>
</div>
<div id="outline-container-orgcace8b9" class="outline-4">
<h4 id="orgcace8b9">MIDI framework</h4>
<div class="outline-text-4" id="text-orgcace8b9">
<p>
We now have a MIDI pipeline and library written, so we can now use this to
create our models (as we&rsquo;ve already begun to some extent).
</p>
</div>
</div>
</div>
<div id="outline-container-org5b81ec6" class="outline-3">
<h3 id="org5b81ec6">Summary of each member</h3>
<div class="outline-text-3" id="text-org5b81ec6">
<ul class="org-ul">
<li><b>Christoffer</b>: Wrote code for flags used in specgan for training. Started
training gansynth specgan on training computer. Kept communication for
access to training resources.</li>
<li><b>Eric</b>: Setup training computer (scripts, environment) and wrote basic
integration tests for our code. Also worked on our implementation of a
transformer.</li>
<li><b>Carl</b>: Work on MIDI tools and get the music transformer repo running.</li>
<li><b>Lovisa</b>: Been busy with other course, but worked on trello planning for the whole group.</li>
<li><b>Cao</b>: Been busy with other course, kept up with work by other group members</li>
<li><b>Elias</b>: Work on gan vae hybrid.</li>
</ul>
</div>
</div>
<div id="outline-container-org28c1eb4" class="outline-3">
<h3 id="org28c1eb4">Next week</h3>
<div class="outline-text-3" id="text-org28c1eb4">
<ul class="org-ul">
<li>Keep writing report</li>
<li>Continue work on music transformer</li>
<li>More extensive training with training computer</li>
<li>Create guide for how to use the training</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgd20b154" class="outline-2">
<h2 id="orgd20b154">Week 7</h2>
<div class="outline-text-2" id="text-orgd20b154">
</div>
<div id="outline-container-org0231d4f" class="outline-3">
<h3 id="org0231d4f">Important info</h3>
<div class="outline-text-3" id="text-org0231d4f">
<p>
We&rsquo;ve migrated to a new drive, which means larger storage capacity but also
means the time log link has been updated to a new link. Our progress will not
be updated on the old link so make sure you check the new one!
</p>

<p>
Also regarding the time log feedback about members not putting in enough
time, due to the IT part of our group having more work to do regarding other
courses, we&rsquo;ve opted for them to only work 16h a week until next week. They
will account for this by working 24h later. We also update the time log every
Friday so if a week is missing
</p>
</div>
</div>

<div id="outline-container-org708f736" class="outline-3">
<h3 id="org708f736">Progress</h3>
<div class="outline-text-3" id="text-org708f736">
</div>
<div id="outline-container-org4cf62f8" class="outline-4">
<h4 id="org4cf62f8">Presentation</h4>
<div class="outline-text-4" id="text-org4cf62f8">
<p>
We held the half time presentation and were satisfied with it, though we
still have some problems we want to work out regarding the scope of the
project.
</p>
</div>
</div>
<div id="outline-container-org06a5f1c" class="outline-4">
<h4 id="org06a5f1c">SpecGAN</h4>
<div class="outline-text-4" id="text-org06a5f1c">
<p>
All we&rsquo;ve done on specGAN this week is to setup training environment and
check pointing so that we can train it for a longer period of time.
</p>

<p>
Below are some results of training the model on all kinds of guitar sounds
in the NSynth dataset. Note that this set includes both acoustic and
electric guitar, which sound very different.
</p>


<div class="figure">
<p><img src="Week%207/result_2020-03-06_12-41-39.gif" alt="result_2020-03-06_12-41-39.gif" />
</p>
</div>

<p>
This is a GIF of the training from epoch 0 to epoch ~140. Not much to say other than it looks decent.
</p>


<div class="figure">
<p><img src="Week%207/image_2020-03-06_12-43-03.png" alt="image_2020-03-06_12-43-03.png" />
</p>
</div>

<p>
This image show a longer training period, epoch ~640 of a different seed. As you
can see, the spectrograms here resemble the real ones calculated in week 5. I
realised I haven&rsquo;t explained how a spectrogram works:
</p>

<ul class="org-ul">
<li>X axis is the sample (time in discrete sense)</li>
<li>Y is the frequency, or tone if you will</li>
<li>Color is the magnitude of the short-term Fourier transform</li>
</ul>

<p>
The straight horizontal lines indicate a frequency or note was played for a long
time. The reason for many horizontal lines are overtones of the note. These
overtones should be evenly spaced, if we are trying to simulate a note from an
instrument. As you can see, the model has far to go in that regard.
</p>

<p>
Also note the purple part to the right. The sound samples are 4 seconds long,
with 64000 samples each but almost all sounds cut out at around 3.2s. That is
way the purple area exists in each spectrogram.
</p>

<p>
I should also mention that this is trained on the valid set of NSynth, meaning
instead of ~280k samples that the training set has, we are only working with
~12k. This is very bad, but the reason has to do with us not being able to load
in the larger dataset into colab due to some bug that is extremely hard to
troubleshoot. (Input/output error if you are curious). There is very little info
online so either we try solving it on our own (no good error log of it) or we
use other training resources.
</p>

<p>
We also have to work on inverting this; there are a lot of parameters that need
to be specified for this inversion to be done correctly and sound okay.
</p>
</div>
</div>

<div id="outline-container-org30cddb7" class="outline-4">
<h4 id="org30cddb7">New model proposal by Elias</h4>
<div class="outline-text-4" id="text-org30cddb7">

<div class="figure">
<p><img src="Week%207/MVIMG_20200306_125637_2020-03-06_13-00-04.jpg" alt="MVIMG_20200306_125637_2020-03-06_13-00-04.jpg" />
</p>
</div>

<p>
While SpecGan is good at generating notes, it is not easy to convert an existing note to a latent vector which can be fed to the generator.  This would be useful if we want to train a network to generate melodies as a sequence of latent space vectors.
</p>

<p>
The solution proposed here is to make a hybrid of variational autoencoders and gans, such that crisp images can still be generated, but it also becomes possible to encode them.
</p>

<p>
The idea is to first train a variational autoencoder, and then train a gan to generate realistic images when given the encoding and some noise as input.
In order to ensure that the generated images look similar to the input, the GAN generated image is also encoded, and the generator
gets an additional loss that ensures that the new encoding is similar to the encoding of the original image.
</p>
</div>
</div>

<div id="outline-container-orgbb7c7d4" class="outline-4">
<h4 id="orgbb7c7d4">Transformer and MIDI</h4>
<div class="outline-text-4" id="text-orgbb7c7d4">
<p>
In the transformer regard, we are working on getting the MIDI pipeline done
so that we can train the transformers on midi data. The dataset for this is
MAESTRO, which includes both raw audio and MIDI of recordings.
</p>

<p>
MIDI is great at structure, and the goal of the transformers are to get long
term structure. Further ahead in the project, we want to combine note
generation with structure of transformers to hopefully generate music with
details of raw audio and structure of MIDI.
</p>

<p>
So far, there&rsquo;s a lot of research about transformers and how other models
have encoded MIDI for use with machine learning.
</p>
</div>
</div>

<div id="outline-container-org343f1a9" class="outline-4">
<h4 id="org343f1a9">Problems</h4>
<div class="outline-text-4" id="text-org343f1a9">
<ul class="org-ul">
<li><b>Resources</b>: Still no reply about resources for training on Chalmers. Sent
another mail asking for a response since it has been a week.</li>
<li><b>Ambitions and scope of project</b>: We will discuss this more in the next
meeting.</li>
<li><b>Low hours Carl</b>: He has 3 other courses that take his time, which makes
distributing the hours difficult.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgbd00766" class="outline-3">
<h3 id="orgbd00766">Summary of each member</h3>
<div class="outline-text-3" id="text-orgbd00766">
<ul class="org-ul">
<li><b>Christoffer</b>: Helped with structuring the presentation. Trained a specGAN to
generate nice looking images (lots of bug testing and hyperparameter tuning
in this task). Minor work on transformers (mostly reading about existing
implementations and how to encode MIDI).</li>
<li><b>Eric</b>: Looked at the MIDI format and created a MIDI encoder function that
can later be used in the dataset preprocessing pipelines. Read about GAN
training techniques like label smoothing. Read about the MIDI format and
created a function to encode MIDI files to a format that can be used to
train a network.</li>
<li><b>Carl</b>: Gave up on wavenet (at least for now), currently working on
preprocessing the MAESTRO dataset)</li>
<li><b>Lovisa</b>: Helped a bit with preparing presentation (along with the rest of
the group), continued work on spectrogram GAN, started working on
transformers with Elias and Christoffer. Mainly tried to get the Music
Transformer by Magenta on GitHub to work, as well as collected some
research relevant to the subject.</li>
<li><b>Cao</b>: Worked on the presentation with the group and presented it with Elias.
Did some light reading about wave2midi2wave.</li>
<li><b>Elias</b>: This week I worked on, and presented the half-time presentation with
cao. Also came up with a new model for encoding and synthesis of high
quality data samples with untangled, normally distributed, latent
representations.</li>
</ul>
</div>
</div>
<div id="outline-container-org3ea7b85" class="outline-3">
<h3 id="org3ea7b85">Next week</h3>
<div class="outline-text-3" id="text-org3ea7b85">
<ul class="org-ul">
<li>We got the recommendation to just work on implementation, but we have quite
a bit of things we could add to the report already.</li>
<li>Finish encoding MIDI and start experimenting with transformers for structure.</li>
<li>Explore the idea described by Elias above</li>
<li>Hopefully solve the resource problem</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orge18c00b" class="outline-2">
<h2 id="orge18c00b">Week 6</h2>
<div class="outline-text-2" id="text-orge18c00b">
<p>
We spent parts of the week revising the project plan, which is now accepted.
</p>
</div>
<div id="outline-container-orgae72b41" class="outline-3">
<h3 id="orgae72b41">Project so far</h3>
<div class="outline-text-3" id="text-orgae72b41">
<p>
The goal for the past two weeks have been generating a note. There has been a
considerable amount of effort put towards this. Below some results are shown
(hard to show audio, we should try hosting those results somewhere and
linking to them)
</p>
</div>

<div id="outline-container-org2aafff1" class="outline-4">
<h4 id="org2aafff1">WaveRNN</h4>
<div class="outline-text-4" id="text-org2aafff1">

<div class="figure">
<p><img src="Week%206/tacotron_wavernn_2020-02-29_11-20-30.png" alt="tacotron_wavernn_2020-02-29_11-20-30.png" />
</p>
</div>


<p>
Eric managed to generate something loosely sounding like a flute using this
model. Loosely as in it&rsquo;s clearly a wind instrument and it is a recognisable
note with overtones but it still needs some work/training.
</p>
</div>
</div>

<div id="outline-container-org3beb8a1" class="outline-4">
<h4 id="org3beb8a1">SpecGAN</h4>
<div class="outline-text-4" id="text-org3beb8a1">
<p>
Unfortunately, the results from this model look decent, but sound terrible.
It doesn&rsquo;t quite follow the implementation specGAN used, so that is an area we could improve.
</p>


<div class="figure">
<p><img src="Week%206/iVBORw0KGg_2020-02-29_11-15-02.png" alt="iVBORw0KGg_2020-02-29_11-15-02.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org2970ca1" class="outline-4">
<h4 id="org2970ca1">WaveNet</h4>
<div class="outline-text-4" id="text-org2970ca1">
<p>
Carl attempted training WaveNet, which when listening could produce both
sine and square waves.
</p>


<div class="figure">
<p><img src="Week%206/1280px-Waveforms.svg_2020-02-29_11-23-23.png" alt="1280px-Waveforms.svg_2020-02-29_11-23-23.png" />
</p>
</div>
</div>
</div>


<div id="outline-container-orgfe5f82e" class="outline-4">
<h4 id="orgfe5f82e">Problems</h4>
<div class="outline-text-4" id="text-orgfe5f82e">
<p>
<b>Too ambitions</b>: The project is very ambitious. The workflow of starting on
simple tasks (generating a note etc) and building on those with sprints
remedies that somewhat. Still, we want to spend some time exactly defining
what the end product will be.
</p>

<p>
<b>Better planning</b>: We&rsquo;ve realised we need a better system for distributing
tasks to the members. Right now you could easily not know what to work. Our
idea is to use Trello for this, but that requires setup and splitting tasks
into even smaller tasks.
</p>

<p>
<b>Resources</b>: We need better resources for training. We&rsquo;ve started asking about
these things. Hopefully we will get an answer next week.
</p>
</div>
</div>
</div>

<div id="outline-container-org756ed74" class="outline-3">
<h3 id="org756ed74">Meetings and workshops</h3>
<div class="outline-text-3" id="text-org756ed74">
<p>
Nothing special, most meetings regarded the project plan, the first
presentation or just working on the two models explained last week.
</p>
</div>
</div>

<div id="outline-container-orga3ee5db" class="outline-3">
<h3 id="orga3ee5db">Summary of each member</h3>
<div class="outline-text-3" id="text-orga3ee5db">
<ul class="org-ul">
<li>Christoffer: Mostly worked on plan and the specGAN model. Also started a
bit on final report and helped with presentation. Also been handling
communication with examiner and sent mails about computing resources</li>
<li>Eric: I started with training an existing model called WaveRNN where I
managed to generate something that sounds like a flute note. I did the
training on my personal computer at home which is not optimal. We need
better computing resources. I then went on to try a model called MelNet,
which is similar to WaveRNN but it uses melspectograms instead of waveforms
which might be more promising.</li>
<li>Carl: Some work on report; successfully training a WaveNet on sine and
square waves</li>
<li>Lovisa: Project plan work, as well as some on the specGAN</li>
<li>Cao: Worked on the presentation, reading about GANSynth, trying out
different discriminator/ generator for the simple GAN model that I
implemented last week.</li>
<li>Elias: Spent the first half of the week rewriting the project plan.
Afterwards I primarily worked on getting a 1d convolutional autoencoder
working. I kind of succeeded, but it is very computationally heavy at the
moment and the loss doesnt really decrease. The output is just noise so
far.</li>
</ul>
</div>
</div>

<div id="outline-container-org4140a1f" class="outline-3">
<h3 id="org4140a1f">Next week</h3>
<div class="outline-text-3" id="text-org4140a1f">
<ul class="org-ul">
<li>Presentation on tuesday</li>
<li>Tweak/train note generation models</li>
<li>Start work on structure models (melody)</li>
<li>Begin writing parts of report (note generation)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org8505c09" class="outline-2">
<h2 id="org8505c09">Week 5</h2>
<div class="outline-text-2" id="text-org8505c09">
<p>
We spent this week working on implementing two kinds of models:
</p>
<ol class="org-ol">
<li>WaveNet - a raw audio generative model mainly used for speech synthesis</li>
<li>SpecGAN - a model using generative adversarial networks for training by converting audio into spectrographs.</li>
</ol>

<p>
The main purpose of this was to generate a note using the NSynth dataset
(dataset consisting of different notes played on different instruments.
</p>
</div>

<div id="outline-container-orga28fd66" class="outline-3">
<h3 id="orga28fd66">Project plan review</h3>
<div class="outline-text-3" id="text-orga28fd66">
<p>
After a meeting with our examiner, there were a fair amount of things that
needed to be changed in the plan.
</p>

<p>
Most of the feedback applies to the entire plan, but here are some key points:
</p>
<ul class="org-ul">
<li><b>Background</b>: Does not explain or motivate the problem well enough. It is meant to capture the reader but our background lacks a lot of passion required for that.</li>
<li><b>Aim</b>: Same here generally, does not explain why this is an important and interesting field.</li>
<li><b>Time plan</b>: Does not tell a story, how will we accomplish these things. Try and detail every week and what happens if we discover hurdles. It also has to detail consistent deliveries, ie if the project suddenly had to stop for whatever reason, what do we have to show for our work?</li>
</ul>

<p>
Deadline for the rewritten plan is Wednesday, <span class="timestamp-wrapper"><span class="timestamp">&lt;2020-02-26 Wed&gt; </span></span> at 12:00. We
will also try to send it to our supervisor by Monday/Tuesday.
</p>
</div>
</div>

<div id="outline-container-orged52b2c" class="outline-3">
<h3 id="orged52b2c">Project so far</h3>
<div class="outline-text-3" id="text-orged52b2c">
<p>
So far, a lot of work has been going on using colab, a notebook editor in
Google drive. It allows limited access to GPUs which makes it great for
smaller experimentation of models. In the future, we&rsquo;ll want to either pay
for access to GPUs, or try and use Chalmers GPU clusters.
</p>
</div>

<div id="outline-container-org8383d61" class="outline-4">
<h4 id="org8383d61">WaveNet</h4>
<div class="outline-text-4" id="text-org8383d61">
<p>
WaveNet requires the amplitudes to be encoded to something that is easier
for the network to work with. This is done using mu<sub>law</sub> encoding, which is
basically just bucketing the amplitudes, but where is gives mode detail to
small amplitudes than large ones.
</p>
</div>
</div>

<div id="outline-container-org2853281" class="outline-4">
<h4 id="org2853281">SpecGAN</h4>
<div class="outline-text-4" id="text-org2853281">
<p>
We were originally going to implement GAN-TTS, but because of its
complexity, we decided to implement something simpler first. As mentioned,
most guides on GANs are for images, so it seemed fitting to start with a
model using images (spectrographs).
</p>


<div id="orgbaddb1f" class="figure">
<p><img src="./img/week5specs.png" alt="week5specs.png" />
</p>
<p><span class="figure-number">Figure 27: </span>Spectrographs for 10 different notes generated</p>
</div>

<p>
This model requires processing the audio waveform into images using digital
signal processing. This did not have to be done manually, as there are
plenty of libraries to use, but the challenge is to ensure all images of the
entire dataset represent the same thing and have the same format and size.
As such, the data preprocessing has been one of the sub tasks for this.
</p>

<p>
The other task is to implement the actual model. There are many guides on
implementing a GAN using the MNIST dataset (dataset consisting of
handwritten letters in image form), but some slight modifications are
required to suit our needs.
</p>
</div>
</div>
</div>

<div id="outline-container-org82e3527" class="outline-3">
<h3 id="org82e3527">Meetings and workshops</h3>
<div class="outline-text-3" id="text-org82e3527">
<p>
Meetings and workshops were spent working on the two models in groups of
three people. Working in groups ensures everyone is learning and are helping
each other.
</p>
</div>
</div>

<div id="outline-container-orgb19c805" class="outline-3">
<h3 id="orgb19c805">Summary of each member</h3>
<div class="outline-text-3" id="text-orgb19c805">
<ul class="org-ul">
<li>Christoffer: Work on the SpecGAN model, specifically the part of converting the entire NSynth dataset into spectrograph images</li>
<li>Eric: Work on preprocessing of data, like using the mu-law algorithm. Also been trying to implement a smaller version of wavenet and learning how to do custom training loops.</li>
<li>Carl: Work on implementing wavenet and rendering the model</li>
<li>Lovisa: Researched and presented sparse transformers. Also worked on the model implementation parts of SpecGAN</li>
<li>Cao: Worked on implementation of the model part of GAN</li>
<li>Elias: Research reformer (efficient transformer) and work a lot on wavenet implementation</li>
</ul>
</div>
</div>

<div id="outline-container-orgb586e02" class="outline-3">
<h3 id="orgb586e02">Next week</h3>
<div class="outline-text-3" id="text-orgb586e02">
<ol class="org-ol">
<li>Complete the project plan</li>
<li>Start basic work on project report</li>
<li>Hopefully generate notes with either of the two models being worked on</li>
<li>If time, start investigating using transformers for the structure part of music generation</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-orgbf4bb2a" class="outline-2">
<h2 id="orgbf4bb2a">Week 4</h2>
<div class="outline-text-2" id="text-orgbf4bb2a">
<p>
Most of this weeks time was spent on planning and writing the project plan.
</p>
</div>

<div id="outline-container-orgf932437" class="outline-3">
<h3 id="orgf932437">Time log warning</h3>
<div class="outline-text-3" id="text-orgf932437">
<p>
Apparently the expected work amount up to (and including) week 3 was an average of
72 hours (according to mail sent to supervisor). Unless this is an error, that
would mean 24 hours worked per week on average. The information we received
was that it&rsquo;s expected to work 20 hours a week, but that initially that is
hard to achieve. In case it&rsquo;s not an error, we are aware of it but it doesn&rsquo;t
match information we&rsquo;ve gotten earlier.
</p>
</div>
</div>

<div id="outline-container-org3c29c23" class="outline-3">
<h3 id="org3c29c23">Regarding project log feedback</h3>
<div class="outline-text-3" id="text-org3c29c23">
<p>
I appreciate the feedback regarding the project log but want to explain something.
So far, most of the work that has been done is either research (paper and
presentation for group), writing contract/plan or minor implementation.
</p>

<p>
I mention this because so far, there&rsquo;s very little to talk about regarding
individual performance here. We could spend a lot of time detailing
everything done, but that is much better done in the time log above. The
point is, up to this point there has been a lot of shared work.
</p>

<p>
Now that the planning stage is over (which is a very shared job), this part
should be easier to write as more individual tasks will be delegated.
</p>
</div>
</div>

<div id="outline-container-org417b8aa" class="outline-3">
<h3 id="org417b8aa">Meetings and workshops</h3>
<div class="outline-text-3" id="text-org417b8aa">
<p>
A meeting with Chalmers writing was booked, but since that required two groups
to sign up, the meeting never went through. We will try to book another one,
but since the plan now is delivered, getting feedback for it seems unneccesary.
</p>

<p>
On Wednesday, the first draft was sent to the supervisor, with feedback
presented to the group on Friday morning. The meeting and workshop held on
Friday was primarily spent on refining the plan after the feedback received.
All in all, the group is happy with how the plan turned out considering the
project is very open and at a slightly more advanced level than common for
bachelor theses.
</p>
</div>
</div>

<div id="outline-container-org77f7527" class="outline-3">
<h3 id="org77f7527">Project so far</h3>
<div class="outline-text-3" id="text-org77f7527">
<p>
The project plan is complete. Some initial trial and error has been
performed, though generating anything close to music is far off. According to
the time plan, we are now in the phase of generating a musical note using
machine learning.
</p>

<p>
A issue we currently face seems to be storage space. Datasets take a fair
amount of space, yet have to be loaded when training. We&rsquo;re currently waiting
for a reply regarding using Chalmers computing clusters but other options are
available at a price. The canvas page does not specify whether pricing for
such clusters are included in the 3000kr budget (as they don&rsquo;t fall under
components or software), so that will have to be investigated.
</p>
</div>
</div>

<div id="outline-container-org5f69de4" class="outline-3">
<h3 id="org5f69de4">Summary of each member</h3>
<div class="outline-text-3" id="text-org5f69de4">
<p>
We will use this section to detail problem solving/tasks delegated to members.
Besides everyone working on the project plan, here are some tasks solved by each member
</p>
<ul class="org-ul">
<li>Christoffer: So far been tasked with documentation, project log writing and generally being the secretary. Otherwise been learning TensorFlow</li>
<li>Eric: Took on the challenge of creating a gantt chart, which he completed by
writing his own JavaScript script. Also have been very active in initial
development and testing of ideas using google colab.</li>
<li>Carl: Ensured our latex documents have proper systems for commenting and change requesting, which helped writing the plan immensely.</li>
<li>Lovisa: Contacted AIVA (AI music company) for info on how their product worked but didn&rsquo;t get much back from them. Also went through TensorFlow guides.</li>
<li>Cao: Research autoencoders and attempted implementing and training basic models using Keras and TensorFlow</li>
<li><p>
Elias: Made an architecture proposal (shown below), which we will look into more next week.
</p></li>
</ul>

<div class="figure">
<p><img src="./img/weekproposal.png" alt="weekproposal.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org3789d5d" class="outline-2">
<h2 id="org3789d5d">Week 3</h2>
<div class="outline-text-2" id="text-org3789d5d">
<p>
As per usual, the week began with a meeting on Tuesday followed by a longer
workshop. During the meeting, the members went through what they had worked on
since last Friday. For the most part, that was research on TensorFlow and a
paper published by Spotify creator group.
</p>

<p>
For the workshop, it was decided that the majority of time
would be spent on writing the project plan. Basic outlining was conducted to
ensure everyone was on the same page regarding the content.
</p>

<p>
On Friday, there was a meeting with the supervisor where the group quickly
went through some research notes they had taken from the presentations held
last week. Additionally the focus of this meeting was on the project plan.
There were a fair amount of criticism of the current rough draft.
</p>

<p>
After this meeting, the rest of the day involved a long workshop on writing
the plan according to the criticism received earlier. A lot was changed and
this brought the draft much closer to the final write up.
</p>

<p>
There is still work to be done on the plan. The deadline is next Friday with
the groups&rsquo; deadline being set to Wednesday. Therefore, the next week will
primarily deal with finishing the project plan.
</p>
</div>

<div id="outline-container-orgc9e65b8" class="outline-3">
<h3 id="orgc9e65b8">Problems encountered</h3>
<div class="outline-text-3" id="text-orgc9e65b8">
<p>
Because the group is not used to writing a research project plan but rather a
product project plan, one of the greatest obstacles have been defining what
will be done. Combined with the wide field, it is difficult to estimate how
much time each task takes.
</p>

<p>
The project task has therefore been simplified a fair bit, but it is still in the
groups ambition to incorporate the more complex features of the project given
that there is available time later on.
</p>
</div>
</div>
</div>

<div id="outline-container-org34ca3c8" class="outline-2">
<h2 id="org34ca3c8">Week 2</h2>
<div class="outline-text-2" id="text-org34ca3c8">
<p>
The week began with a meeting on Tuesday, during which a number of points were brought up
</p>
<ul class="org-ul">
<li>Decide report language and register that on canvas</li>
<li>Began talk about the project report</li>
<li>Discussions on the current write up of the contract</li>
</ul>

<p>
The meeting was immediately followed by a workshop, where how to efficiently
structure out research was determined. we concluded that the
group would divide into subgroups with the intent of each reading and
summarising papers. Machine learning is a wide field, beyond basic concepts,
learning everything will take away too much time from the actual project.
</p>

<p>
After a meeting with the supervisor on Friday, a research meeting was held.
The idea was to take the subgroups determined earlier and have them present
their findings for the group. This process will be evaluated for future
research meetings, but we felt it was a good start. If anything, the primary
goal of them is to spark discussions, which it was very effective at.
</p>

<p>
Because Cao only returned on Thursday, the contract wasn&rsquo;t sent to our
supervisor until Friday evening, after the meeting. The contract is now
considered finished.
</p>

<p>
Though stated in last weeks log that we would begin work on the project plan
this week, small strides were made in that direction. This has a lot to do
with the very open project description. The primary hurdle is to decide on a
goal that is not too easy, but realistic enough to achieve. With such a wide
field and different ways of doing things, we have given that part a bit more time.
</p>

<p>
Next week will be focused on the project plan and another research meeting.
</p>
</div>
</div>

<div id="outline-container-org2978513" class="outline-2">
<h2 id="org2978513">Week 1</h2>
<div class="outline-text-2" id="text-org2978513">
<p>
Since this is the first week of the project, the majority of it has been
discussing the project and reading up on research papers. We started the week
by attending the introductory seminars.
</p>

<p>
During the three meetings, we set up a slack group, had our first meeting with the supervisor and
started writing the group contract.
</p>

<p>
Alone, most of us studied research papers. Since some of the members lacked
experience in the field, Elias set up a notebook intended for teaching the
basics.
</p>

<p>
For personal reasons, Cao was absent for part of the week, but this was notified well in advance.
</p>

<p>
For next week, we are looking to finish the group contract, continue
researching and starting work on the project plan
</p>
</div>
</div>
</div>
</body>
</html>
