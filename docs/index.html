<?xml version="1.0" encoding="utf-8"?>
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2020-04-09 Thu 11:08 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Project Log</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Christoffer Arvidsson" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/styles/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:1f739d935676111cfff4b4693e3816e664797050&amp;dn=gpl-3.0.txt GPL-v3-or-Later
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.cacheClassElem = elem.className;
         elem.cacheClassTarget = target.className;
         target.className = "code-highlighted";
         elem.className   = "code-highlighted";
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(elem.cacheClassElem)
         elem.className = elem.cacheClassElem;
       if(elem.cacheClassTarget)
         target.className = elem.cacheClassTarget;
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Project Log</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org7d1997f">Introduction</a></li>
<li><a href="#org1538b9d">Week 12</a>
<ul>
<li><a href="#org954a351">Progress</a>
<ul>
<li><a href="#org2e7f536">Pitch conditioning and Auxillary Classifier</a></li>
<li><a href="#org7b86fb3">Training</a></li>
<li><a href="#org2a39f4d">Current Results of Note Generation with Conditioning and Classifier</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org035613b">Week 11</a>
<ul>
<li><a href="#orgc964936">Progress</a>
<ul>
<li><a href="#org6e5a816">Report</a></li>
<li><a href="#org3df8f61">Note generation</a></li>
<li><a href="#org6be5dce">Structure generation</a></li>
<li><a href="#orgcce3896">Training</a></li>
</ul>
</li>
<li><a href="#org1fafa4a">Last weeks&rsquo; goals</a></li>
<li><a href="#orgf1db33c">Summary of each member</a></li>
<li><a href="#orgbfb1880">Next week</a></li>
</ul>
</li>
<li><a href="#org7013f9b">Week 10</a>
<ul>
<li><a href="#org267b249">Progress</a>
<ul>
<li><a href="#orgcf6cfbe">Report</a></li>
<li><a href="#org44fcf23">Note Generation</a></li>
<li><a href="#org28162b2">Structure Generation</a></li>
<li><a href="#orga88eee1">Bayes Training</a></li>
</ul>
</li>
<li><a href="#orgdc50d79">Summary of each member</a></li>
<li><a href="#org162d4d6">Next week</a></li>
</ul>
</li>
<li><a href="#org9a2e046">Week 9</a>
<ul>
<li><a href="#orgf91b887">Progress</a>
<ul>
<li><a href="#orge8c7ef2">Report</a></li>
<li><a href="#org00df620">Shift of focus</a></li>
<li><a href="#orge994ce3">VAE</a></li>
<li><a href="#org9b09702">Transformer</a></li>
<li><a href="#orgcc130e4">Training resources</a></li>
<li><a href="#orgee0e0b3">Meetings</a></li>
<li><a href="#org7c64f1d">Exam week</a></li>
</ul>
</li>
<li><a href="#org733f0db">Next week</a></li>
</ul>
</li>
<li><a href="#orgfa59b14">Week 8</a>
<ul>
<li><a href="#orgdb75522">Progress</a>
<ul>
<li><a href="#org5ec3b84">Training resources</a></li>
<li><a href="#orga10ddc4">Meetings</a></li>
<li><a href="#org96d7a2d">Exam week</a></li>
<li><a href="#org40ef6db">MIDI framework</a></li>
</ul>
</li>
<li><a href="#org730df71">Summary of each memeber</a></li>
<li><a href="#orgbae8a70">Next week</a></li>
</ul>
</li>
<li><a href="#orge2fbc49">Week 7</a>
<ul>
<li><a href="#org4b38bca">Important info</a></li>
<li><a href="#orgf25c8d5">Progress</a>
<ul>
<li><a href="#org9dafa21">Presentation</a></li>
<li><a href="#orgd2a136f">SpecGAN</a></li>
<li><a href="#org0180af2">New model proposal by Elias</a></li>
<li><a href="#org7860e08">Transformer and MIDI</a></li>
<li><a href="#orgb4cbae9">Problems</a></li>
</ul>
</li>
<li><a href="#org3ecdd74">Summary of each member</a></li>
<li><a href="#orgd800c84">Next week</a></li>
</ul>
</li>
<li><a href="#orge29473d">Week 6</a>
<ul>
<li><a href="#org68ed0bd">Project so far</a>
<ul>
<li><a href="#org64887bc">WaveRNN</a></li>
<li><a href="#orgcf24315">SpecGAN</a></li>
<li><a href="#org5389429">WaveNet</a></li>
<li><a href="#org9aa5de4">Problems</a></li>
</ul>
</li>
<li><a href="#org424e217">Meetings and workshops</a></li>
<li><a href="#org10a6e9a">Summary of each member</a></li>
<li><a href="#org13e9312">Next week</a></li>
</ul>
</li>
<li><a href="#org437c4db">Week 5</a>
<ul>
<li><a href="#orgc663fb8">Project plan review</a></li>
<li><a href="#orgd3c0cb5">Project so far</a>
<ul>
<li><a href="#orge3b8f4f">WaveNet</a></li>
<li><a href="#org3a98f08">SpecGAN</a></li>
</ul>
</li>
<li><a href="#org9484a90">Meetings and workshops</a></li>
<li><a href="#orgc9bcfa5">Summary of each member</a></li>
<li><a href="#org74ca44e">Next week</a></li>
</ul>
</li>
<li><a href="#org3a2defb">Week 4</a>
<ul>
<li><a href="#orgb99ebad">Time log warning</a></li>
<li><a href="#org97e82c3">Regarding project log feedback</a></li>
<li><a href="#org5876a63">Meetings and workshops</a></li>
<li><a href="#orge21ab07">Project so far</a></li>
<li><a href="#org41fd8f2">Summary of each member</a></li>
</ul>
</li>
<li><a href="#orgd9fb8dd">Week 3</a>
<ul>
<li><a href="#org71adf1d">Problems encountered</a></li>
</ul>
</li>
<li><a href="#org13f2fcb">Week 2</a></li>
<li><a href="#org745cf1a">Week 1</a></li>
</ul>
</div>
</div>

<div id="outline-container-org7d1997f" class="outline-2">
<h2 id="org7d1997f">Introduction</h2>
<div class="outline-text-2" id="text-org7d1997f">
<p>
This page is a shared log detailing the work the group has done and what
challenges and problems it has encountered. For the most part, it&rsquo;s updated
weekly unless some big change happens.
</p>

<p>
A time log can be found at Google Sheets: <a href="https://bit.ly/2PRKRl8">https://bit.ly/2PRKRl8</a>
</p>

<p>
The time log has exact tasks worked on each entry, for each
</p>
</div>
</div>
<div id="outline-container-org1538b9d" class="outline-2">
<h2 id="org1538b9d">Week 12</h2>
<div class="outline-text-2" id="text-org1538b9d">
</div>
<div id="outline-container-org954a351" class="outline-3">
<h3 id="org954a351">Progress</h3>
<div class="outline-text-3" id="text-org954a351">
<p>
We are currently working hard on getting pitch conditioning to work since that
is the bottleneck for connecting with structure. To accomplish this, we are
constantly reading about ways of conditioning GANs, one of which is using an
<i>auxillary classifier</i>.
</p>
</div>

<div id="outline-container-org2e7f536" class="outline-4">
<h4 id="org2e7f536">Pitch conditioning and Auxillary Classifier</h4>
<div class="outline-text-4" id="text-org2e7f536">
<p>
As explained last week, we require this step to be able to choose which notes
are being played. However, from our research and from limited testing, we found
that the generator rarely if ever made use of the additonal pitch information.
To encourage the generator to actually use it, we implement an auxillary
classifier loss to the network.
</p>


<div id="orgd64ebac" class="figure">
<p><img src="./img/aux_class_overview.png" alt="aux_class_overview.png" />
</p>
<p><span class="figure-number">Figure 1: </span>Overview of how a Auxillary Classifier GAN works (ACGAN)</p>
</div>

<p>
The <i>ACGAN</i> in figure <a href="#orgd64ebac">1</a> shows pitch class as \(c\), noise input as
\(z\), and pitch embedding as \(x\). shows pitch class as \(c\), noise input as \(z\),
and the real image as x (which also needs a pitch class)
</p>

<p>
Without explaining exactly how a GAN work, there are two networks, a generator
and a discriminator. The aux classifier comes in at the discriminator, who
normally only has the job of determining fake vs real images from its input
(either an image from the dataset or form the generator). With an aux
classifier, there is a second loss, depending on if the discriminator can
correctly label the pitch of the generated image.
</p>

<p>
The discriminators (D) job is then to maximize \[L_{D} = L_{C} + L_{S}\] where
\(L_{C}\) is the log-likelyhood of correct class (or pitch in this case), and
\(L_{S}\) is the log-likelyhood of correct source (real vs fake image).
</p>

<p>
The generators (G) job is then to maximise \[L_{G} = L_{C} - L_{S}\] or in
words, get D to determine the class label right, but fail at determining real vs
fake.
</p>

<p>
With an actual loss to the discriminator and generator, the hope is they won&rsquo;t
ignore the pitch vectors anymore. We have almost implemented all this, but are
still experimenting with getting it entirely correct., but are still
experimenting with getting it entirely correct.
</p>
</div>
</div>

<div id="outline-container-org7b86fb3" class="outline-4">
<h4 id="org7b86fb3">Training</h4>
<div class="outline-text-4" id="text-org7b86fb3">
<p>
Our required time to train has gone up drasticly. For example, the pitch
conditioning with an auxillary classifier (but probably without one as well)
required 2.5 days to train. That is around 1.5M steps. We may have been able
to decrease this but that is still a lot of time spent on waiting for results
that may or may not be great. This is after we decreased each epoch (~200 steps)
from 40 seconds to 11 seconds by caching all images from the dataset into memory.
</p>


<div id="orgc6302d9" class="figure">
<p><img src="./img/aux_class_training.png" alt="aux_class_training.png" />
</p>
<p><span class="figure-number">Figure 2: </span>Training of Aux classifier, shows the Nash Equilibrium. The skips are due to pausing training to try generating.</p>
</div>

<p>
Figure <a href="#orgc6302d9">2</a> shows that it does reach an equilibrium, which is where the
training is optimal. It does reach this stage, but determining when to stop is a
difficult thing. Another issue that arises with longer training periods is
overfitting on the dataset, which is not idea either.
</p>
</div>
</div>

<div id="outline-container-org2a39f4d" class="outline-4">
<h4 id="org2a39f4d">Current Results of Note Generation with Conditioning and Classifier</h4>
<div class="outline-text-4" id="text-org2a39f4d">
<p>
The following results were taken from the above training session at the last
skip and at the end. (steps 600K and 1500K). The pitches used are increasing.
</p>


<div id="org8fef3e9" class="figure">
<p><img src="./img/600k_steps.png" alt="600k_steps.png" />
</p>
<p><span class="figure-number">Figure 3: </span>Result at 600K steps</p>
</div>

<p>
<audio controls="controls" src="audio/600k_steps.wav"></audio>
</p>


<div id="orgb8827c7" class="figure">
<p><img src="./img/1500k_steps.png" alt="1500k_steps.png" />
</p>
<p><span class="figure-number">Figure 4: </span>Result at 1500K steps</p>
</div>

<p>
<audio controls="controls" src="audio/1500k_steps.wav"></audio>
</p>

<p>
The main issue in this seems to be mode collapse, the generator no matter pitch
or noise input generates the same or similiar examples. In the case of figure
<a href="#orgb8827c7">4</a>, there are three kidns of images, of which only one of them
resembles a keyboard (which is what we are training on). Arguably, you could
have noticed mode collapse in figure <a href="#org8fef3e9">3</a>, but we thought we would
give it a try anyway.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org035613b" class="outline-2">
<h2 id="org035613b">Week 11</h2>
<div class="outline-text-2" id="text-org035613b">
<p>
No great audio results for this week unfortunately.
</p>
</div>
<div id="outline-container-orgc964936" class="outline-3">
<h3 id="orgc964936">Progress</h3>
<div class="outline-text-3" id="text-orgc964936">
</div>
<div id="outline-container-org6e5a816" class="outline-4">
<h4 id="org6e5a816">Report</h4>
<div class="outline-text-4" id="text-org6e5a816">
<p>
We have made good progress on the report, though we feel that since our
implementation is so advanced compared to the typical bachelor thesis, the
theory section has to be really large. As such, we have primarily been writing
theory and a bit on implementation.
</p>

<p>
We came up with a good structure with our supervisor and are right now focusing
on making each chapter have roughly the same amount of pages. We could easily
write too much for this report, so we have to find a balance of what is actually
required to explain to understand the implementation. An example is: how do you
explain how a transformer with attention work? This topic could easily take ~20
pages but leaving it too short would make understanding difficult for the reader
who we target as our coursemates.
</p>
</div>
</div>

<div id="outline-container-org3df8f61" class="outline-4">
<h4 id="org3df8f61">Note generation</h4>
<div class="outline-text-4" id="text-org3df8f61">
<p>
In order to connect this component to the final system, we need to condition it
on pitch so we can controlling which pitch/note the generator generates. This is
what we have been working on this week. The main idea is to append a one-hot
vector representation of notes (size of 88 which are the number of keys on a
standard piano or number of tones in MIDI). The hope is that the generator then
learns to generate random sounding tones according to that vector.
</p>

<p>
Pitch conditioning is almost done but we cannot be sure it is entirely right.
We may have to add another loss component to the discriminator called an
<i>auxillary classifier</i> that would encourage the generator to care more about the
conditioning vector.
</p>

<p>
This idea came from the GANSynth paper. They do a lot of other things as well,
including adding a phase channel to the network, allowing it to generate its own
phase instead of having to reconstruct it using the Griffin-lim algorithm. They
call this Instantaneous Frequency, which is the derivative of phase. We also
want to implement this because even though our processing nad inversion sounds
&ldquo;good enough&rdquo;, this could improve it.
</p>

<p>
Due to limited training, we do not have a lot of great sounding examples yet so
but hopefully we will have some by next week.
</p>
</div>
</div>

<div id="outline-container-org6be5dce" class="outline-4">
<h4 id="org6be5dce">Structure generation</h4>
<div class="outline-text-4" id="text-org6be5dce">
<p>
Structure generation is since last week pretty much done, maybe we could train
it a bit more but our main concern is fixing note generation so that we can
connect the two.
</p>
</div>
</div>

<div id="outline-container-orgcce3896" class="outline-4">
<h4 id="orgcce3896">Training</h4>
<div class="outline-text-4" id="text-orgcce3896">
<p>
We started using Bayes but we are not quite sure if we are using it correctly.
The canvas course page states to ensure there are not more threads running than
designated CPU cores, so the main concern is configuring tensorflow to not spawn
~200 threads.
</p>

<p>
We also set up tensorboard to better log our results. We will most likely start
auto generating our images and audio samples but it&rsquo;s not a priority.
</p>
</div>
</div>
</div>

<div id="outline-container-org1fafa4a" class="outline-3">
<h3 id="org1fafa4a">Last weeks&rsquo; goals</h3>
<div class="outline-text-3" id="text-org1fafa4a">
<ul class="org-ul">
<li>We did some extensive training on Bayes</li>
<li>Spectrogram processing is for the most part done (there are still some issues
with normalization)</li>
<li>Wasserstein not implemented yet due to wanting to focus on processing, who we
deemd to be the thing making the note generation sound off.</li>
<li>Posterior collapse still an issue</li>
<li>We&rsquo;ve come a good way with the report but there is a lot of theory</li>
</ul>
</div>
</div>
<div id="outline-container-orgf1db33c" class="outline-3">
<h3 id="orgf1db33c">Summary of each member</h3>
<div class="outline-text-3" id="text-orgf1db33c">
<ul class="org-ul">
<li><b>Christoffer</b>:
<ul class="org-ul">
<li>Spent most of my time structuring and writing the report.</li>
<li>Also worked with Eric and Elias to implement pitch conditioning on the
GAN.</li>
<li>Also helped debug our spectrogram processing though most of that work was
Eric.</li>
<li>Been really digging into the GANSynth details, even though they don&rsquo;t do
a great job of explaining their model.</li>
<li>Finally, did some training on Bayes and attempted to configure
tensorflow/numpy to only use x amount of threads.</li>
</ul></li>
<li><b>Eric</b>:
<ul class="org-ul">
<li>I mostly worked on the VAE(which is starting to look like a dead-end to
me)</li>
<li>Worked on GAN to add the pitch conditioning and fix normalization.</li>
</ul></li>
<li><b>Carl</b>:
<ul class="org-ul">
<li>Wrote on a few sections including ethics and music transformer, rewrote
midi pipeline</li>
</ul></li>
<li><b>Lovisa</b>:
<ul class="org-ul">
<li>Working on Wasserstein GAN and the math behind it.</li>
<li>Have been reviewing github issues and added content to the report.</li>
</ul></li>
<li><b>Cao</b>:
<ul class="org-ul">
<li>Did some reading and expanded some of the subsections for the final
report.</li>
<li>Also reviewed the specgan/ pipeline for more understanding so id be able
to help with writing the implementation section/ documentation for the
code.</li>
</ul></li>
<li><p>
<b>Elias</b>:
</p>
<ul class="org-ul">
<li>Worked on getting pitch conditioning to work together with Christoffer and</li>
</ul>
<p>
Eric.
</p>
<ul class="org-ul">
<li>Also worked on implemented an alternative GAN training scheme which</li>
</ul>
<p>
has some similarities to Wasserstein GAN.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-orgbfb1880" class="outline-3">
<h3 id="orgbfb1880">Next week</h3>
<div class="outline-text-3" id="text-orgbfb1880">
<ul class="org-ul">
<li>Auxillary classifier for the SpecGAN</li>
<li>Maybe incorporate WGAN if results are not great</li>
<li>Work on connecting structure and note, even if note is not on par with
structure yet.</li>
<li>Keep adding to the report, particularly the implementation sections.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org7013f9b" class="outline-2">
<h2 id="org7013f9b">Week 10</h2>
<div class="outline-text-2" id="text-org7013f9b">
</div>
<div id="outline-container-org267b249" class="outline-3">
<h3 id="org267b249">Progress</h3>
<div class="outline-text-3" id="text-org267b249">
</div>
<div id="outline-container-orgcf6cfbe" class="outline-4">
<h4 id="orgcf6cfbe">Report</h4>
<div class="outline-text-4" id="text-orgcf6cfbe">
<p>
More and more chapters are being handled, specifically the theory parts and
techniques we have used over the course of the project. There is also a complete
structure that makes it easy to add content. We have also been thinking about
how we present our journey in the report, and decided that we would add an
experiments section.
</p>
</div>
</div>
<div id="outline-container-org44fcf23" class="outline-4">
<h4 id="org44fcf23">Note Generation</h4>
<div class="outline-text-4" id="text-org44fcf23">
<p>
We are currently trying to implement a Wasserstein GAN, which should improve the
results of the GAN. Below are some audio files that have been processed back
from spectrograms generated by our model.
</p>

<p>
Generated results after training on the GANSynth dataset (slight changes from the NSynth
dataset) . <b>Warning, these are quite loud!</b>
</p>

<p>
<audio controls="controls" src="audio/our_inverted_specs_fixed.wav"></audio>
</p>

<p>
They do not sound that good but some tonality is there. We think there may be a
problem with the inversion back into audio. To demonstrate this, the following
audio snippet is from real notes inverted into spectrograms and then back with
the same data pipeline:
</p>

<p>
<audio controls="controls" src="audio/actual_inverted_spec_fixed.wav"></audio>
</p>

<p>
There are similiar results indicating that there is an issue. Whether this is
due to errors in spectrogram generation or from inverting back, we have not
concluded yet.
</p>

<p>
A real inversion (with all parameters set properly) will not sound perfect due
to processing audio into a spectrogram throws away the phase information of the
signal. There are algorithms for rebuilding this (Griffith-lim) but they are not
perfect. Early tests in the project showed that the quality is good enough, much
better than the results we get now.
</p>
</div>
</div>

<div id="outline-container-org28162b2" class="outline-4">
<h4 id="org28162b2">Structure Generation</h4>
<div class="outline-text-4" id="text-org28162b2">
<p>
Last week, the results sounded quite good. That is because the prior was very
long, the model ended up copying too much resulting in an existing song (with
some minor alterations). We think it may be due to overfitting on the dataset
(MAESTRO). The following audio snippet was generated with a smaller prior:
</p>

<p>
<audio controls="controls" src="audio/mutrans_half_prior_half_gen.wav"></audio>
</p>

<p>
We believe the MIDI encoding is fine (good enough) since encoding and decoding a
real song gives good results (besides it not being lossless).
</p>
</div>
</div>

<div id="outline-container-orga88eee1" class="outline-4">
<h4 id="orga88eee1">Bayes Training</h4>
<div class="outline-text-4" id="text-orga88eee1">
<p>
We finally started training on Bayes; the first round is currently scheduled to
train 50 epochs of our VAE implementation. We do not yet have any results of this.
</p>
</div>
</div>
</div>
<div id="outline-container-orgdc50d79" class="outline-3">
<h3 id="orgdc50d79">Summary of each member</h3>
<div class="outline-text-3" id="text-orgdc50d79">
<ul class="org-ul">
<li><b>Christoffer</b>:
<ul class="org-ul">
<li>Wrote initial draft for a datasets chapter, explaining the NSynth and MAESTRO datasets</li>
<li>Wrote an entire theory chapter on signal processing</li>
<li>Read up on how to use Bayes for scheduling running tasks</li>
<li>Review of pull requests and issues in our git report repo</li>
<li>Did some basic reading on Wasserstein GANs, but nothing major.</li>
<li>Looked into getting Tensorboard to work (required us to uninstall other versions of tensorflow.</li>
</ul></li>
<li><b>Eric</b>:
<ul class="org-ul">
<li>Wrote on experiments section in report</li>
<li>Tried to figure out why the transformer is copying the prior with a lot of trial and error</li>
<li>Tried hyperparameter optimization</li>
</ul></li>
<li><b>Carl</b>:
<ul class="org-ul">
<li>Some infrastructure and tech support</li>
<li>Work on report, mainly overall style and front pages, but also some content</li>
<li>Made some plots with pgfplots, but probably going to give that up</li>
</ul></li>
<li><b>Lovisa</b>:
<ul class="org-ul">
<li>Took over some of the email communications, have been writing to
Arne Linde about our computer at chalmers and communicating with examinator
and supervisor too. Tried to be mor active on github, reading comments and
making more reviews. Also kept working on wgan and the report.</li>
</ul></li>
<li><b>Cao</b>:
<ul class="org-ul">
<li>Read about RNN, LSTM, Transformer.</li>
<li>Made documentation for the transformer model.</li>
<li>Started writing subsections for the final report: What is ML and AI
models, Variational autoencoder and Deep neural networks.</li>
</ul></li>
<li><b>Elias</b>:
<ul class="org-ul">
<li>Added ability to generate samples of real and autoencoded audio samples
to the vae gan. Found that the results are very different so spent a lot
of time tweaking hyperparameters, modifying the training algortihm and
training to improve the results. Still no good results unfortunately.</li>
<li>Also wrote on the report. Specifically about transformers, wavenet, and
musenet.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org162d4d6" class="outline-3">
<h3 id="org162d4d6">Next week</h3>
<div class="outline-text-3" id="text-org162d4d6">
<ul class="org-ul">
<li>More extensive training of models on Bayes</li>
<li>Fix spectrogram processing</li>
<li>Implement Wasserstein GAN (already in progress)</li>
<li>Fix posterior collapse in the VAE (similiar to mode collapse in GANS)</li>
<li>Keep adding content to report</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org9a2e046" class="outline-2">
<h2 id="org9a2e046">Week 9</h2>
<div class="outline-text-2" id="text-org9a2e046">
</div>
<div id="outline-container-orgf91b887" class="outline-3">
<h3 id="orgf91b887">Progress</h3>
<div class="outline-text-3" id="text-orgf91b887">
</div>
<div id="outline-container-orge8c7ef2" class="outline-4">
<h4 id="orge8c7ef2">Report</h4>
<div class="outline-text-4" id="text-orge8c7ef2">
<p>
We created a detailed outline of sections in the report that will make it
easy to add things as we develop them. We&rsquo;ve also started writing parts of the
Theory chapter, explaining basic concepts. Finally, we want to write about the
models we&rsquo;ve implemented and tested.
</p>
</div>
</div>

<div id="outline-container-org00df620" class="outline-4">
<h4 id="org00df620">Shift of focus</h4>
<div class="outline-text-4" id="text-org00df620">
<p>
We decided to shift our focus entirely to the transformer and variational auto
encoder since we felt the wave2midi2wave wouldn&rsquo;t pan out in a way we would
hope.
</p>
</div>
</div>

<div id="outline-container-orge994ce3" class="outline-4">
<h4 id="orge994ce3">VAE</h4>
<div class="outline-text-4" id="text-orge994ce3">
<p>
Elias and cao were assigned to this task, but since exams, not a lot of progress
in this area wasmade this week. The model has however had similiar results to
SpecGan, so we are still researching this.
</p>
</div>
</div>

<div id="outline-container-org9b09702" class="outline-4">
<h4 id="org9b09702">Transformer</h4>
<div class="outline-text-4" id="text-org9b09702">
<p>
The transformer aims to deal with the structure of music. It trains on MIDI,
learning the relationships between sequences of MIDI notes and outputs the most
appropriate notes. The aim of this model is to connect it with the note
generation to generate complete music.
</p>

<p>
We&rsquo;ve been working towards getting the music transformer model running and also
implementing our own version of it. This week, we managed to run both and
generate results. Below are the audio snippets of the two.
</p>

<p>
<b>Our music transformer with prior</b>
</p>

<p>
<audio controls="controls" src="audio/mutrans_our_prior.wav"></audio>
</p>

<p>
<audio controls="controls" src="audio/mutrans_our_prior2.wav"></audio>
</p>

<p>
<b>Music transformer implementation found online without prior</b>
</p>

<p>
<audio controls="controls" src="audio/mutrans_test.wav"></audio>
</p>

<p>
As you can hear, all three examples have structure, which is promising. We
will continue working on these and later connecting it with our note generation!
</p>
</div>
</div>

<div id="outline-container-orgcc130e4" class="outline-4">
<h4 id="orgcc130e4">Training resources</h4>
<div class="outline-text-4" id="text-orgcc130e4">
<p>
We got access to another training platform, Bayes at DS&amp;AI division. This server
has much better hardware than the previous one, but also restrictions when it comes
to time slots to train and amount of training. We&rsquo;ll eventually use it to most
likely train the transformer since it requires better specs than we had.
</p>
</div>
</div>

<div id="outline-container-orgee0e0b3" class="outline-4">
<h4 id="orgee0e0b3">Meetings</h4>
<div class="outline-text-4" id="text-orgee0e0b3">
<p>
Meetings has been going well online, we try to work more in voice calls and limit
how much we meet in person. It is still challenging ensuring everyone has tasks
to work on and ensuring everyone is on the same page.
</p>
</div>
</div>

<div id="outline-container-org7c64f1d" class="outline-4">
<h4 id="org7c64f1d">Exam week</h4>
<div class="outline-text-4" id="text-org7c64f1d">
<p>
Still exam week so some members haven&rsquo;t gotten a lot donech memeber
</p>
<ul class="org-ul">
<li><b>Christoffer</b>: Work on issues in the github like structuring repo, Also
structured and started writing the report (signal processing in theory). A
lot of the time is writing scripts to generate plots we can use in the
report. Wrote guide on how to use training computer.</li>
<li><b>Eric</b>: Work on transformer, refactoring and general implementation details
(refactor project and split parts of code into separate runnable scripts).
Big issue for transformers is memory to train for long sequences and the
model copying itâ€™s prior (initial input).</li>
<li><b>Carl</b>: Deploy script, refactoring and reviewing github pull requests. Wrote
a progressbar module for our training scripts, showing progress of training.</li>
<li><b>Lovisa</b>: Skeleton/outline of report and also started on implementing
wasserstein loss for specgan(math heavy so complete study mode of the
math). Progress in understanding the subject so will add to report. Had
trouble setting up repo (our repo) on laptop, carl helped with that.</li>
<li><b>Cao</b>: Set up things for the remote computer. Has been busy with other
courses to really participate much. Also as been missing an assigned task
which us in the group are working on fixing for next week.</li>
<li><b>Elias</b>: Not a lot of work since last meeting, mostly focused on other course
due to exam week.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org733f0db" class="outline-3">
<h3 id="org733f0db">Next week</h3>
<div class="outline-text-3" id="text-org733f0db">
<ul class="org-ul">
<li>Keep writing report</li>
<li>Continue work on music transformer</li>
<li>More extensive training with training computer and potentially Bayes</li>
<li>Create more throrough tests (unit and integration) (from last week)</li>
<li>Write a bunch of utility functions (flags, plotting etc). (from last week)</li>
<li>Continue work on the VAE and maybe begin connecting everything</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgfa59b14" class="outline-2">
<h2 id="orgfa59b14">Week 8</h2>
<div class="outline-text-2" id="text-orgfa59b14">
</div>
<div id="outline-container-orgdb75522" class="outline-3">
<h3 id="orgdb75522">Progress</h3>
<div class="outline-text-3" id="text-orgdb75522">
<p>
In general, because of the pandemic and exams, the project progressed less
than other weeks. There have been some progress with audio generation, but it
is hard to include audio snippets into this page so maybe they will exist in
our repo at some point.
</p>
</div>
<div id="outline-container-org5ec3b84" class="outline-4">
<h4 id="org5ec3b84">Training resources</h4>
<div class="outline-text-4" id="text-org5ec3b84">
<p>
We finally gained access to a computer we can use for training. This means a
lot of our time was spent on setup of this computer and porting of our colab
code to work on it.
</p>
</div>
</div>
<div id="outline-container-orga10ddc4" class="outline-4">
<h4 id="orga10ddc4">Meetings</h4>
<div class="outline-text-4" id="text-orga10ddc4">
<p>
Due to the pandemic, we may start holding meetings online rather than in
person (if multiple people message about not being able to join).
Supervision meetings are all held online for now on until further notice
from Chalmers.
</p>
</div>
</div>
<div id="outline-container-org96d7a2d" class="outline-4">
<h4 id="org96d7a2d">Exam week</h4>
<div class="outline-text-4" id="text-org96d7a2d">
<p>
Because it is exam time for other courses, a lot of group members had to
spend their time studying for those or writing reports.
</p>
</div>
</div>
<div id="outline-container-org40ef6db" class="outline-4">
<h4 id="org40ef6db">MIDI framework</h4>
<div class="outline-text-4" id="text-org40ef6db">
<p>
We now have a MIDI pipeline and library written, so we can now use this to
create our models (as we&rsquo;ve already begun to some extent).
</p>
</div>
</div>
</div>
<div id="outline-container-org730df71" class="outline-3">
<h3 id="org730df71">Summary of each memeber</h3>
<div class="outline-text-3" id="text-org730df71">
<ul class="org-ul">
<li><b>Christoffer</b>: Wrote code for flags used in specgan for training. Started
training gansynth specgan on training computer. Kept communication for
access to training resources.</li>
<li><b>Eric</b>: Setup training computer (scripts, environment) and wrote basic
integration tests for our code. Also worked on our implementation of a
transformer.</li>
<li><b>Carl</b>: Work on MIDI tools and get the music transformer repo running.</li>
<li><b>Lovisa</b>: Been busy with other course, but worked on trello planning for the whole group.</li>
<li><b>Cao</b>: Been busy with other course, kept up with work by other gorup memebers</li>
<li><b>Elias</b>: Work on gan vae hybrid.</li>
</ul>
</div>
</div>
<div id="outline-container-orgbae8a70" class="outline-3">
<h3 id="orgbae8a70">Next week</h3>
<div class="outline-text-3" id="text-orgbae8a70">
<ul class="org-ul">
<li>Keep writing report</li>
<li>Continue work on music transformer</li>
<li>More extensive training with training computer</li>
<li>Create guide for how to use the training</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orge2fbc49" class="outline-2">
<h2 id="orge2fbc49">Week 7</h2>
<div class="outline-text-2" id="text-orge2fbc49">
</div>
<div id="outline-container-org4b38bca" class="outline-3">
<h3 id="org4b38bca">Important info</h3>
<div class="outline-text-3" id="text-org4b38bca">
<p>
We&rsquo;ve migrated to a new drive, which means larger storage capacity but also
means the timelog link has been updated to a new link. Our progress will not
be updated on the old link so make sure you check the new one!
</p>

<p>
Also regarding the time log feedback about members not putting in enough
time, due to the IT part of our group having more work to do regarding other
courses, we&rsquo;ve opted for them to only work 16h a week until next week. They
will account for this by working 24h later. We also update the time log every
Friday so if a week is missing
</p>
</div>
</div>

<div id="outline-container-orgf25c8d5" class="outline-3">
<h3 id="orgf25c8d5">Progress</h3>
<div class="outline-text-3" id="text-orgf25c8d5">
</div>
<div id="outline-container-org9dafa21" class="outline-4">
<h4 id="org9dafa21">Presentation</h4>
<div class="outline-text-4" id="text-org9dafa21">
<p>
We held the half time presentation and were satisfied with it, though we
still have some problems we want to work out regarding the scope of the
project.
</p>
</div>
</div>
<div id="outline-container-orgd2a136f" class="outline-4">
<h4 id="orgd2a136f">SpecGAN</h4>
<div class="outline-text-4" id="text-orgd2a136f">
<p>
All we&rsquo;ve done on specGAN this week is to setup training environment and
checkpointing so that we can train it for a longer period of time.
</p>

<p>
Below are some results of training the model on all kinds of guitar sounds
in the NSynth dataset. Note that this set includes both acoustic and
electric guitar, which sound very different.
</p>


<div class="figure">
<p><img src="Week%207/result_2020-03-06_12-41-39.gif" alt="result_2020-03-06_12-41-39.gif" />
</p>
</div>

<p>
This is a GIF of the training from epoch 0 to epoch ~140. Not much to say other than it looks decent.
</p>


<div class="figure">
<p><img src="Week%207/image_2020-03-06_12-43-03.png" alt="image_2020-03-06_12-43-03.png" />
</p>
</div>

<p>
This image show a longer training period, epoch ~640 of a different seed. As you
can see, the spectrograms here resemble the real ones calculated in week 5. I
realised I haven&rsquo;t explained how a spectrogram works:
</p>

<ul class="org-ul">
<li>X axis is the sample (time in discrete sense)</li>
<li>Y is the frequency, or tone if you will</li>
<li>Color is the magnitude of the short-term fourier transform</li>
</ul>

<p>
The straight horizontal lines indicate a frequency or note was played for a long
time. The reason for many horizontal lines are overtones of the note. These
overtones should be evenly spaced, if we are trying to simulate a note from an
instrument. As you can see, the model has far to go in that regard.
</p>

<p>
Also note the purple part to the right. The sound samples are 4 seconds long,
with 64000 samples each but almost all sounds cut out at around 3.2s. That is
way the purple area exists in each spectrogram.
</p>

<p>
I should also mention that this is trained on the valid set of NSynth, meaning
instead of ~280k samples that the training set has, we are only working with
~12k. This is very bad, but the reason has to do with us not being able to load
in the larger dataset into colab due to some bug that is extremely hard to
troubleshoot. (Input/output error if you are curious). There is very little info
online so either we try solving it on our own (no good error log of it) or we
use other training resources.
</p>

<p>
We also have to work on inverting this; there are a lot of parameters that need
to be specified for this inversion to be done correctly and sound okay.
</p>
</div>
</div>

<div id="outline-container-org0180af2" class="outline-4">
<h4 id="org0180af2">New model proposal by Elias</h4>
<div class="outline-text-4" id="text-org0180af2">

<div class="figure">
<p><img src="Week%207/MVIMG_20200306_125637_2020-03-06_13-00-04.jpg" alt="MVIMG_20200306_125637_2020-03-06_13-00-04.jpg" />
</p>
</div>

<p>
While SpecGan is good at generating notes, it is not easy to convert an existing note to a latent vector which can be fed to the generator.  This would be useful if we want to train a network to generate melodies as a sequence of latent space vectors.
</p>

<p>
The solution proposed here is to make a hybrid of variational autoencoders and gans, such that crisp images can still be generated, but it also becomes possible to encode them.
</p>

<p>
The idea is to first train a variational autoencoder, and then train a gan to generate realistic images when given the encoding and some noise as input.
In order to ensure that the generated images look similar to the input, the GAN generated image is also encoded, and the generator
gets an additional loss that ensures that the new encoding is similar to the encoding of the original image.
</p>
</div>
</div>

<div id="outline-container-org7860e08" class="outline-4">
<h4 id="org7860e08">Transformer and MIDI</h4>
<div class="outline-text-4" id="text-org7860e08">
<p>
In the transoformer regard, we are working on getting the MIDI pipeline done
so that we can train the transformers on midi data. The dataset for this is
MAESTRO, which includes both raw audio and MIDI of recordings.
</p>

<p>
MIDI is great at structure, and the goal of the transformers are to get long
term structure. Further ahead in the project, we want to combine note
generation with structure of transformers to hopefully generate music with
details of raw audio and structure of MIDI.
</p>

<p>
So far, there&rsquo;s a lot of research about transformers and how other models
have encoded MIDI for use with machine learning.
</p>
</div>
</div>

<div id="outline-container-orgb4cbae9" class="outline-4">
<h4 id="orgb4cbae9">Problems</h4>
<div class="outline-text-4" id="text-orgb4cbae9">
<ul class="org-ul">
<li><b>Resources</b>: Still no reply about resources for training on chalmers. Sent
another mail asking for a response since it has been a week.</li>
<li><b>Ambitions and scope of project</b>: We will discuss this more in the next
meeting.</li>
<li><b>Low hours carl</b>: He has 3 other courses that take his time, which makes
distributing the hours difficult.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org3ecdd74" class="outline-3">
<h3 id="org3ecdd74">Summary of each member</h3>
<div class="outline-text-3" id="text-org3ecdd74">
<ul class="org-ul">
<li><b>Christoffer</b>: Helped with structuring the presentation. Trained a specGAN to
generate nice looking images (lots of bug testing and hyperparameter tuning
in this task). Minor work on transformers (mostly reading about existing
implementations and how to encode MIDI).</li>
<li><b>Eric</b>: Looked at the MIDI format and created a MIDI encoder function that
can later be used in the dataset preprocessing pipelines. Read about GAN
training techniques like label smoothing. Read about the MIDI format and
created a function to encode MIDI files to a format that can be used to
train a network.</li>
<li><b>Carl</b>: Gave up on wavenet (at least for now), currently working on
preprocessing the MAESTRO dataset)</li>
<li><b>Lovisa</b>: Helped a bit with preparing presentation (along with the rest of
the group), continued work on spectrogram GAN, started working on
transformers with Elias and Christoffer. Mainly tried to get the Music
Transformer by Magenta on github to work, as well as collected some
research relevant to the subject.</li>
<li><b>Cao</b>: Worked on the presentation with the group and presented it with Elias.
Did some light reading about wave2midi2wave.</li>
<li><b>Elias</b>: This week I worked on, and presented the half-time presentation with
cao. Also came up with a new model for encoding and synthesis of high
quality data samples with untangled, normally distributed, latent
representations.</li>
</ul>
</div>
</div>
<div id="outline-container-orgd800c84" class="outline-3">
<h3 id="orgd800c84">Next week</h3>
<div class="outline-text-3" id="text-orgd800c84">
<ul class="org-ul">
<li>We got the recommendation to just work on implementation, but we have quite
a bit of things we could add to the report already.</li>
<li>Finish encoding MIDI and start experimenting with transformers for structure.</li>
<li>Explore the idea described by Elias above</li>
<li>Hopefully solve the resource problem</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orge29473d" class="outline-2">
<h2 id="orge29473d">Week 6</h2>
<div class="outline-text-2" id="text-orge29473d">
<p>
We spent parts of the week revising the project plan, which is now accepted.
</p>
</div>
<div id="outline-container-org68ed0bd" class="outline-3">
<h3 id="org68ed0bd">Project so far</h3>
<div class="outline-text-3" id="text-org68ed0bd">
<p>
The goal for the past two weeks have been generating a note. There has been a
considerable amount of effort put towards this. Below some results are shown
(hard to show audio, we should try hosting those results somewhere and
linking to them)
</p>
</div>

<div id="outline-container-org64887bc" class="outline-4">
<h4 id="org64887bc">WaveRNN</h4>
<div class="outline-text-4" id="text-org64887bc">

<div class="figure">
<p><img src="Week%206/tacotron_wavernn_2020-02-29_11-20-30.png" alt="tacotron_wavernn_2020-02-29_11-20-30.png" />
</p>
</div>


<p>
Eric managed to generate something loosely sounding like a flute using this
model. Loosely as in it&rsquo;s clearly a wind instrument and it is a recognizable
note with overtones but it still needs some work/training.
</p>
</div>
</div>

<div id="outline-container-orgcf24315" class="outline-4">
<h4 id="orgcf24315">SpecGAN</h4>
<div class="outline-text-4" id="text-orgcf24315">
<p>
Unfortuneately, the results from this model look decent, but sound terrible.
It doesn&rsquo;t quite follow the implementation specGAN used, so that is an area we could improve.
</p>


<div class="figure">
<p><img src="Week%206/iVBORw0KGg_2020-02-29_11-15-02.png" alt="iVBORw0KGg_2020-02-29_11-15-02.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org5389429" class="outline-4">
<h4 id="org5389429">WaveNet</h4>
<div class="outline-text-4" id="text-org5389429">
<p>
Carl attempted training WaveNet, which when listening could produce both
sine and square waves.
</p>


<div class="figure">
<p><img src="Week%206/1280px-Waveforms.svg_2020-02-29_11-23-23.png" alt="1280px-Waveforms.svg_2020-02-29_11-23-23.png" />
</p>
</div>
</div>
</div>


<div id="outline-container-org9aa5de4" class="outline-4">
<h4 id="org9aa5de4">Problems</h4>
<div class="outline-text-4" id="text-org9aa5de4">
<p>
<b>Too ambitions</b>: The project is very ambitious. The workflow of starting on
simple tasks (generating a note etc) and building on those with sprints
remedies that somewhat. Still, we want to spend some time exactly defining
what the end product will be.
</p>

<p>
<b>Better planning</b>: We&rsquo;ve realised we need a better system for distributing
tasks to the members. Right now you could easily not know what to work. Our
idea is to use Trello for this, but that requires setup and splitting tasks
into even smaller tasks.
</p>

<p>
<b>Resources</b>: We need better resources for training. We&rsquo;ve started asking about
these things. Hopefully we will get an answer next week.
</p>
</div>
</div>
</div>

<div id="outline-container-org424e217" class="outline-3">
<h3 id="org424e217">Meetings and workshops</h3>
<div class="outline-text-3" id="text-org424e217">
<p>
Nothing special, most meetings regarded the project plan, the first
presentation or just working on the two models explained last week.
</p>
</div>
</div>

<div id="outline-container-org10a6e9a" class="outline-3">
<h3 id="org10a6e9a">Summary of each member</h3>
<div class="outline-text-3" id="text-org10a6e9a">
<ul class="org-ul">
<li>Christoffer: Mostly worked on plan and the specGAN model. Also started a
bit on final report and helped with presentation. Also been handling
communication wih examiner and sent mails about computing resources</li>
<li>Eric: I started with training an existing model called WaveRNN where I
managed to generate something that sounds like a flute note. I did the
training on my personal computer at home which is not optimal. We need
better computing resources. I then went on to try a model called MelNet,
which is similar to WaveRNN but it uses melspectograms instead of waveforms
which might be more promising.</li>
<li>Carl: Some work on report; successfully training a WaveNet on sine and
square waves</li>
<li>Lovisa: Project plan work, as well as some on the specGAN</li>
<li>Cao: Worked on the presentation, reading about GANSynth, trying out
different discriminator/ generator for the simple GAN model that I
implemented last week.</li>
<li>Elias: Spent the first half of the week rewriting the project plan.
Afterwards I primarily worked on getting a 1d convolutional autoencoder
working. I kind of succeeded, but it is very computationally heavy at the
moment and the loss doesnâ€™t really decrease. The output is just noise so
far.</li>
</ul>
</div>
</div>

<div id="outline-container-org13e9312" class="outline-3">
<h3 id="org13e9312">Next week</h3>
<div class="outline-text-3" id="text-org13e9312">
<ul class="org-ul">
<li>Presentation on tuesday</li>
<li>Tweak/train note generation models</li>
<li>Start work on structure models (melody)</li>
<li>Begin writing parts of report (note generation)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org437c4db" class="outline-2">
<h2 id="org437c4db">Week 5</h2>
<div class="outline-text-2" id="text-org437c4db">
<p>
We spent this week working on implementing two kinds of models:
</p>
<ol class="org-ol">
<li>WaveNet - a raw audio generative model mainly used for speech synthesis</li>
<li>SpecGAN - a model using generative adversarial networks for training by converting audio into spectrographs.</li>
</ol>

<p>
The main purpose of this was to generate a note using the NSynth dataset
(dataset consisting of different notes played on different instruments.
</p>
</div>

<div id="outline-container-orgc663fb8" class="outline-3">
<h3 id="orgc663fb8">Project plan review</h3>
<div class="outline-text-3" id="text-orgc663fb8">
<p>
After a meeting with our examiner, there were a fair amount of things that
needed to be changed in the plan.
</p>

<p>
Most of the feedback applies to the entire plan, but here are some key points:
</p>
<ul class="org-ul">
<li><b>Background</b>: Does not explain or motivate the problem well enough. It is meant to capture the reader but our background lacks a lot of passion required for that.</li>
<li><b>Aim</b>: Same here generally, does not explain why this is an important and interesting field.</li>
<li><b>Timeplan</b>: Does not tell a story, how will we accomplish these things. Try and detail every week and what happens if we discover hurdles. It also has to detail consistent deliveries, ie if the project suddenly had to stop for whatever reason, what do we have to show for our work?</li>
</ul>

<p>
Deadline for the rewritten plan is Wednesday, <span class="timestamp-wrapper"><span class="timestamp">&lt;2020-02-26 Wed&gt; </span></span> at 12:00. We
will also try to send it to our supervisor by Monday/Tuesday.
</p>
</div>
</div>

<div id="outline-container-orgd3c0cb5" class="outline-3">
<h3 id="orgd3c0cb5">Project so far</h3>
<div class="outline-text-3" id="text-orgd3c0cb5">
<p>
So far, a lot of work has been going on using colab, a notebook editor in
Google drive. It allows limited access to GPUs which makes it great for
smaller experimentation of models. In the future, we&rsquo;ll want to either pay
for access to GPUs, or try and use Chalmers GPU clusters.
</p>
</div>

<div id="outline-container-orge3b8f4f" class="outline-4">
<h4 id="orge3b8f4f">WaveNet</h4>
<div class="outline-text-4" id="text-orge3b8f4f">
<p>
WaveNet requires the amplitudes to be encoded to something that is easier
for the network to work with. This is done using mu<sub>law</sub> encoding, which is
basically just bucketing the amplitudes, but where is gives mode detail to
small amplitudes than large ones.
</p>
</div>
</div>

<div id="outline-container-org3a98f08" class="outline-4">
<h4 id="org3a98f08">SpecGAN</h4>
<div class="outline-text-4" id="text-org3a98f08">
<p>
We were originally going to implement GAN-TTS, but because of its
complexity, we decided to implement something simpler first. As mentioned,
most guides on GANs are for images, so it seemed fitting to start with a
model using images (spectrographs).
</p>


<div id="org16c2f16" class="figure">
<p><img src="./img/week5specs.png" alt="week5specs.png" />
</p>
<p><span class="figure-number">Figure 11: </span>Spectrographs for 10 different notes generated</p>
</div>

<p>
This model requires processing the audio waveform into images using digital
signal processing. This did not have to be done manually, as there are
plenty of libraries to use, but the challenge is to ensure all images of the
entire dataset represent the same thing and have the same format and size.
As such, the data preprocessing has been one of the subtasks for this.
</p>

<p>
The other task is to implement the actual model. There are many guides on
implementing a GAN using the MNIST dataset (dataset consisting of
handwritten letters in image form), but some slight modifications are
required to suit our needs.
</p>
</div>
</div>
</div>

<div id="outline-container-org9484a90" class="outline-3">
<h3 id="org9484a90">Meetings and workshops</h3>
<div class="outline-text-3" id="text-org9484a90">
<p>
Meetings and workshops were spent working on the two models in groups of
three people. Working in groups ensures everyone is learning and are helping
eachother.
</p>
</div>
</div>

<div id="outline-container-orgc9bcfa5" class="outline-3">
<h3 id="orgc9bcfa5">Summary of each member</h3>
<div class="outline-text-3" id="text-orgc9bcfa5">
<ul class="org-ul">
<li>Christoffer: Work on the SpecGAN model, specifically the part of converting the entire NSynth dataset into spectrograph images</li>
<li>Eric: Work on preprocessing of data, like using the mu-law algorithm. Also been trying to implement a smaller version of wavenet and learning how to do custom training loops.</li>
<li>Carl: Work on implementing wavenet and rendering the model</li>
<li>Lovisa: Researched and presented sparse transformers. Also worked on the model implementation parts of SpecGAN</li>
<li>Cao: Worked on implementation of the model part of GAN</li>
<li>Elias: Research reformer (efficient transformer) and work a lot on wavenet implementation</li>
</ul>
</div>
</div>

<div id="outline-container-org74ca44e" class="outline-3">
<h3 id="org74ca44e">Next week</h3>
<div class="outline-text-3" id="text-org74ca44e">
<ol class="org-ol">
<li>Complete the project plan</li>
<li>Start basic work on project report</li>
<li>Hopefully generate notes with either of the two models being worked on</li>
<li>If time, start investigating using transformers for the structure part of music generation</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org3a2defb" class="outline-2">
<h2 id="org3a2defb">Week 4</h2>
<div class="outline-text-2" id="text-org3a2defb">
<p>
Most of this weeks time was spent on planning and writing the project plan.
</p>
</div>

<div id="outline-container-orgb99ebad" class="outline-3">
<h3 id="orgb99ebad">Time log warning</h3>
<div class="outline-text-3" id="text-orgb99ebad">
<p>
Apparently the expected work amount up to (and including) week 3 was an average of
72 hours (according to mail sent to supervisor). Unless this is an error, that
would mean 24 hours worked per week on average. The information we received
was that it&rsquo;s expected to work 20 hours a week, but that initially that is
hard to achieve. In case it&rsquo;s not an error, we are aware of it but it doesn&rsquo;t
match information we&rsquo;ve gotten earlier.
</p>
</div>
</div>

<div id="outline-container-org97e82c3" class="outline-3">
<h3 id="org97e82c3">Regarding project log feedback</h3>
<div class="outline-text-3" id="text-org97e82c3">
<p>
I appreciate the feedback regarding the project log but want to explain something.
So far, most of the work that has been done is either research (paper and
presentation for group), writing contract/plan or minor implementation.
</p>

<p>
I mention this because so far, there&rsquo;s very little to talk about regarding
individual performance here. We could spend a lot of time detailing
everything done, but that is much better done in the time log above. The
point is, up to this point there has been a lot of shared work.
</p>

<p>
Now that the planning stage is over (which is a very shared job), this part
should be easier to write as more individual tasks will be delegated.
</p>
</div>
</div>

<div id="outline-container-org5876a63" class="outline-3">
<h3 id="org5876a63">Meetings and workshops</h3>
<div class="outline-text-3" id="text-org5876a63">
<p>
A meeting with chalmers writing was booked, but since that required two groups
to sign up, the meeting never went through. We will try to book another one,
but since the plan now is delivered, getting feedback for it seems unneccesary.
</p>

<p>
On wednesday, the first draft was sent to the supervisor, with feedback
presented to the group on friday morning. The meeting and workshop held on
friday was primarily spent on refining the plan after the feedback received.
All in all, the group is happy with how the plan turned out considering the
project is very open and at a slightly more advanced level than common for
bachelor theses.
</p>
</div>
</div>

<div id="outline-container-orge21ab07" class="outline-3">
<h3 id="orge21ab07">Project so far</h3>
<div class="outline-text-3" id="text-orge21ab07">
<p>
The project plan is complete. Some initial trial and error has been
performed, though generating anything close to music is far off. According to
the timeplan, we are now in the phase of generating a musical note using
machine learning.
</p>

<p>
A issue we currently face seems to be storage space. Datasets take a fair
amount of space, yet have to be loaded when training. We&rsquo;re currently waiting
for a reply regarding using Chalmers computing clusters but other options are
available at a price. The canvas page does not specify whether pricing for
such clusters are included in the 3000kr budget (as they don&rsquo;t fall under
components or software), so that will have to be investigated.
</p>
</div>
</div>

<div id="outline-container-org41fd8f2" class="outline-3">
<h3 id="org41fd8f2">Summary of each member</h3>
<div class="outline-text-3" id="text-org41fd8f2">
<p>
We will use this section to detail problemsolving/tasks delegated to members.
Besides everyone working on the project plan, here are some tasks solved by each member
</p>
<ul class="org-ul">
<li>Christoffer: So far been tasked with documentation, project log writing and generally being the secretary. Otherwise been learning tensorflow</li>
<li>Eric: Took on the challenge of creating a gantt chart, which he completed by
writing his own javascript script. Also have been very active in initial
development and testing of ideas using google colab.</li>
<li>Carl: Ensured our latex documents have proper systems for commenting and change requesting, which helped writing the plan immensely.</li>
<li>Lovisa: Contacted AIVA (AI music company) for info on how their product worked but didn&rsquo;t get much back from them. Also went through tensorflow guides.</li>
<li>Cao: Research autoencoders and attempted implementing and training basic models using Keras and tensorflow</li>
<li><p>
Elias: Made an architecture proposal (shown below), which we will look into more next week.
</p></li>
</ul>

<div class="figure">
<p><img src="./img/weekproposal.png" alt="weekproposal.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgd9fb8dd" class="outline-2">
<h2 id="orgd9fb8dd">Week 3</h2>
<div class="outline-text-2" id="text-orgd9fb8dd">
<p>
As per usual, the week began with a meeting on Tuesday followed by a longer
workshop. During the meeting, the members went through what they had worked on
since last friday. For the most part, that was research on tensorflow and a
paper published by Spotify creator group.
</p>

<p>
For the workshop, it was decided that the majority of time
would be spent on writing the project plan. Basic outlining was conducted to
ensure everyone was on the same page regarding the content.
</p>

<p>
On friday, there was a meeting with the supervisor where the group quickly
went through some research notes they had taken from the presentations held
last week. Additionally the focus of this meeting was on the project plan.
There were a fair amount of criticism of the current rough draft.
</p>

<p>
After this meeting, the rest of the day involved a long workshop on writing
the plan according to the criticism received earlier. A lot was changed and
this brought the draft much closer to the final writeup.
</p>

<p>
There is still work to be done on the plan. The deadline is next friday with
the groups&rsquo; deadline being set to Wednesday. Therefore, the next week will
primarily deal with finishing the project plan.
</p>
</div>

<div id="outline-container-org71adf1d" class="outline-3">
<h3 id="org71adf1d">Problems encountered</h3>
<div class="outline-text-3" id="text-org71adf1d">
<p>
Because the group is not used to writing a research project plan but rather a
product project plan, one of the greatest obstacles have been defining what
will be done. Combined with the wide field, it is difficult to estimate how
much time each task takes.
</p>

<p>
The project task has therefore been simplified a fair bit, but it is still in the
groups ambition to incorporate the more complex features of the project given
that there is available time later on.
</p>
</div>
</div>
</div>

<div id="outline-container-org13f2fcb" class="outline-2">
<h2 id="org13f2fcb">Week 2</h2>
<div class="outline-text-2" id="text-org13f2fcb">
<p>
The week began with a meeting on tuesday, during which a number of points were brought up
</p>
<ul class="org-ul">
<li>Decide report language and register that on canvas</li>
<li>Began talk about the project report</li>
<li>Discussions on the current writeup of the contract</li>
</ul>

<p>
The meeting was immediately followed by a workshop, where how to efficiently
structure out research was determined. we concluded that the
group would divide into subgroups with the intent of each reading and
summarizing papers. Machine learning is a wide field, beyond basic concepts,
learning everything will take away too much time from the actual project.
</p>

<p>
After a meeting with the supervisor on friday, a research meeting was held.
The idea was to take the subgroups determined earlier and have them present
their findings for the group. This process will be evaluated for future
research meetings, but we felt it was a good start. If anything, the primary
goal of them is to spark discussions, which it was very effective at.
</p>

<p>
Because Cao only returned on thursday, the contract wasn&rsquo;t sent to our
supervisor until Friday evening, after the meeting. The contract is now
considered finished.
</p>

<p>
Though stated in last weeks log that we would begin work on the project plan
this week, small strides were made in that direction. This has a lot to do
with the very open project description. The primary hurdle is to decide on a
goal that is not too easy, but realistic enough to achieve. With such a wide
field and different ways of doing things, we have given that part a bit more time.
</p>

<p>
Next week will be focused on the project plan and another research meeting.
</p>
</div>
</div>

<div id="outline-container-org745cf1a" class="outline-2">
<h2 id="org745cf1a">Week 1</h2>
<div class="outline-text-2" id="text-org745cf1a">
<p>
Since this is the first week of the project, the majority of it has been
discussing the project and reading up on research papers. We started the week
by attending the introductory seminars.
</p>

<p>
During the three meetings, we set up a slack group, had our first meeting with the supervisor and
started writing the group contract.
</p>

<p>
Alone, most of us studied research papers. Since some of the members lacked
experience in the field, Elias set up a notebook intended for teaching the
basics.
</p>

<p>
For personal reasons, Cao was absent for part of the week, but this was notified well in advance.
</p>

<p>
For next week, we are looking to finish the group contract, continue
researching and starting work on the project plan
</p>
</div>
</div>
</div>
</body>
</html>
